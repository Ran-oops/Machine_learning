{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7dfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "DELETE FROM B;\n",
    "INSERT INTO B\n",
    "SELECT * FROM A;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "梯度是一个向量，它表示一个多元函数在某个点处的方向导数沿着该方向取得最大值。即函数在改点处沿着该方向(此梯度的方向)变化最快，变化率最大(\n",
    "为该梯度的模)\n",
    "\n",
    "向量的模可以理解为是向量的长度，向量的模是只有大小没有方向的。假设空间向量v (x,y,z)，其中x,y,z分别是三轴上的坐标。\n",
    "由计算模长的公式可得：|v|=²√（x²+y²+z²）\n",
    "对于n维向量v=(x1,x2,…,xn)，其模的计算公式为|v|=²√(x1²+x2²+…+xn²)。这个公式可以推广到任意维度的向量\n",
    "假设我们有一个三维空间中的向量v (3,4,5)，其中x=3，y=4，z=5。根据计算模长的公式，|v|=²√（x²+y²+z²）=²√（3²+4²+5²）=²√（50）≈7.07。\n",
    "所以这个向量的模长约为7.07\n",
    "\n",
    "模的概念在数学和物理学中都有重要的应用。在数学中，模可以用来衡量向量的大小，也可以用来计算两个向量之间的距离。在物理学中，\n",
    "模可以用来表示物理量的大小，\n",
    "例如速度、力和加速度等。此外，在机器学习和深度学习中，梯度下降算法中也会用到模的概念\n",
    "\n",
    "如果我们有两个向量a和b，那么它们之间的距离可以通过计算向量a-b的模来得到。具体来说，如果a=(x1,y1,z1)和b=(x2,y2,z2)，\n",
    "那么a和b之间的距离为|a-b|=²√((x1-x2)²+(y1-y2)²+(z1-z2)²)。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "梯度下降算法是一种常用的优化算法，它通过迭代来最小化一个目标函数。在每次迭代中，算法会沿着目标函数梯度的负方向移动一定的步长，直到\n",
    "收敛到最小值\n",
    "\n",
    "在梯度下降算法中，模的概念可以用来确定步长，步长决定了每次迭代中参数更新的幅度，它通常与梯度的模成正比，也就是说，当梯度的模较大时，\n",
    "步长也会相应地变大，从而加快收敛速度，而当梯度的模较小时，步长也会相应地变小，从而提高收敛的精度。此外，在某些情况下，我们还可以使用模\n",
    "来确定迭代终止条件。例如，当梯度的模小于某个阈值时，我们可以认为算法已经收敛，并终止迭代。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6263f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "算法是怎么做到沿着梯度负方向移动呢？\n",
    "答：通过更新参数； 梯度下降算法通过更新参数来沿着梯度的负方向移动。具体来说，假设我们由一个目标函数f(x),其中x是一个向量，表示模型的参数。\n",
    "每次迭代时，我们会计算目标函数关于x的梯度，然后用这个梯度来更新参数x,更新公式为：x = x -α * ∇f(x), 其中α是步长，∇f(x)表示目标函数关于\n",
    "x的梯度。 由于梯度指向目标函数增长最快的方向，所以梯度的负方向就指向目标函数式减小最快的方向。因此，通过沿着梯度的负方向更新参数，我们就\n",
    "可以使目标函数逐渐较小，直到收敛到最小值\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ab7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "举个实际的例子：\n",
    "假设我们有一个简单的线性回归问题，目标是找到一条直线来拟合一组数据点，我们可以定义目标函数为均方误差， 即f(w,b)=1/n * Σ(yi-(wx+b))²。其中\n",
    "w和b分别表示直线的斜率和截距，n表示数据点的个数，yi表示第i个数据点的y坐标。\n",
    "在这个例子中，我们的参数向量x=(w,b),目标函数关于x的梯度为∇f(x)=(∂f/∂w, ∂f/∂b)。我们可以通过求导来计算梯度的每个分量，具体来说，\n",
    "∂f/∂w=2/n * Σ(yi-(wx+b)) * (-x)=-2/n * Σ(yi-(wx+b)) * x，∂f/∂b=2/n * Σ(yi-(wx+b)) * (-1)=-2/n * Σ(yi-(wx+b))\n",
    "\n",
    "∂f/∂w就是偏导数\n",
    "在每次迭代中，我们先计算梯度，然后用梯度来更新参数w和b,更新公式为：w = w - α * ∂f/∂w\n",
    "b = b - α * ∂f/∂b。\n",
    "通过不断迭代，我们就可以找到一组最优的参数w和b,使得目标函数达到最小值\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea06184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "迭代是什么意思？\n",
    "迭代是指重复执行同一过程，直到满足某个终止条件为止。 在梯度下降算法中，迭代通常包括以下步骤：\n",
    "1. 计算目标函数关于参数的梯度\n",
    "2. 根据梯度和步长来更新参数\n",
    "3. 检查是否满足终止条件。如果满足，则终止迭代，否则，返回第一步。\n",
    "\n",
    "终止条件可以有很多种，例如迭代次数达到一定值，目标函数的变化量小于某个阈值，梯度的模小于某个阈值等。具体的终止条件取决于问题本身和\n",
    "算法的实现。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee9a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1.97078250602932, b: 0.06641823135781669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n我们定义了目标函数f（均方误差）， 梯度计算函数grad（用来计算目标函数关于参数w和b的梯度）和梯度下降算法gradient_descent. 在这个函数中，\\n我们初始化参数w和b为0，然后进行迭代，在每次迭代中，我们先计算梯度，然后用梯度来更新参数w和b，最后我们返回最优的参数w和b。\\n\\ndv和db是梯度， 表示的是目标函数关于参数w和b的偏导数，我们使用梯度来更新参数w和b，更新公式为：\\nw = w - α * dw，b = b - α * db\\n其中α表示步长，它决定了每次迭代中参数更新的幅度。\\n\\n以上代码中没有使用到模来确定步长，而是将步长α设为了一个固定的值0.01， 这种做法简单易实现。但可能不是最优的。\\n在实际应用中，我们通常会使用更复杂的方法来确定步长，以提高算法的性能，例如，我们可以使用模来确定步长，也可以使用线搜索或者信頼域方法\\n来动态调整步长。这些方法可以根据目标函数的性质和当前迭代的状态来确定最优的步长。\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单线性回归问题，梯度下降算法的示例代码\n",
    "import numpy as np\n",
    "\n",
    "# 定义目标函数\n",
    "def f(w, b, X, Y):\n",
    "    return np.mean((Y - (w * X + b)) ** 2)\n",
    "\n",
    "# 定义梯度计算函数\n",
    "def grad(w, b, X, Y):\n",
    "    # np.mean((Y - (w * X + b)) * X)就相当于1/n * Σ(yi-(wx+b)) * x\n",
    "    dw = -2 * np.mean((Y - (w * X + b)) * X)\n",
    "    db = -2 * np.mean(Y - (w * X + b))\n",
    "    return dw, db\n",
    "\n",
    "# 定义梯度下降算法\n",
    "def gradient_descent(X, Y, alpha=0.01, n_iters=1000):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    for i in range(n_iters):\n",
    "        dw, db = grad(w, b, X, Y)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        # print(\"w\", w)\n",
    "        # print(\"b\",b)\n",
    "    return w, b\n",
    "\n",
    "# 测试数据\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([2, 4, 6])\n",
    "\n",
    "# 运行梯度下降算法\n",
    "w, b = gradient_descent(X, Y)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"w: {w}, b: {b}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "我们定义了目标函数f（均方误差）， 梯度计算函数grad（用来计算目标函数关于参数w和b的梯度）和梯度下降算法gradient_descent. 在这个函数中，\n",
    "我们初始化参数w和b为0，然后进行迭代，在每次迭代中，我们先计算梯度，然后用梯度来更新参数w和b，最后我们返回最优的参数w和b。\n",
    "\n",
    "dv和db是梯度， 表示的是目标函数关于参数w和b的偏导数，我们使用梯度来更新参数w和b，更新公式为：\n",
    "w = w - α * dw，b = b - α * db\n",
    "其中α表示步长，它决定了每次迭代中参数更新的幅度。\n",
    "\n",
    "以上代码中没有使用到模来确定步长，而是将步长α设为了一个固定的值0.01， 这种做法简单易实现。但可能不是最优的。\n",
    "在实际应用中，我们通常会使用更复杂的方法来确定步长，以提高算法的性能，例如，我们可以使用模来确定步长，也可以使用线搜索或者信頼域方法\n",
    "来动态调整步长。这些方法可以根据目标函数的性质和当前迭代的状态来确定最优的步长。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81893d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1.9999352766020413, b: -2.8471933849476357e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n使用了一个新的变量step_size来表示步长，步长的值由α除以梯度的模来确定。即step_size = alpha / np.sqrt(dw ** 2 + db ** 2)，\\n这样，当梯度的模较大时，步长也会相应地变大，当梯度的模较小时，步长也会相应地变小。\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单线性回归问题，梯度下降算法的示例代码\n",
    "# 使用模来确定步长\n",
    "import numpy as np\n",
    "\n",
    "# 定义目标函数\n",
    "def f(w, b, X, Y):\n",
    "    return np.mean((Y - (w * X + b)) ** 2)\n",
    "\n",
    "# 定义梯度计算函数\n",
    "def grad(w, b, X, Y):\n",
    "    # np.mean((Y - (w * X + b)) * X)就相当于1/n * Σ(yi-(wx+b)) * x\n",
    "    dw = -2 * np.mean((Y - (w * X + b)) * X)\n",
    "    db = -2 * np.mean(Y - (w * X + b))\n",
    "    return dw, db\n",
    "\n",
    "# 定义梯度下降算法\n",
    "def gradient_descent(X, Y, alpha=0.0001, n_iters=100000):\n",
    "    w = 0\n",
    "    b = 0\n",
    "    for i in range(n_iters):\n",
    "        dw, db = grad(w, b, X, Y)\n",
    "        step_size = alpha / np.sqrt(dw ** 2 + db ** 2)\n",
    "        w -= step_size * dw\n",
    "        b -= step_size * db\n",
    "    return w, b\n",
    "\n",
    "# 测试数据\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([2, 4, 6])\n",
    "\n",
    "# 运行梯度下降算法\n",
    "w, b = gradient_descent(X, Y)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"w: {w}, b: {b}\")\n",
    "\n",
    "\"\"\"\n",
    "使用了一个新的变量step_size来表示步长，步长的值由α除以梯度的模来确定。即step_size = alpha / np.sqrt(dw ** 2 + db ** 2)，\n",
    "这样，当梯度的模较大时，步长也会相应地变大，当梯度的模较小时，步长也会相应地变小。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor是Torch包的核心类，如果将其属性.requires_grad=True，它将开始追踪在其上的所有操作，这样就可以利用链式法则进行梯度传播了，\n",
    "完成计算后，可以调用.backward()来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。如果不想要被追踪，可以调用.detach()将其从\n",
    "追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用with torch.no_grad()将不想被追踪的操作\n",
    "代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数的梯度(requires_grad=True)\n",
    "\n",
    "注意在y.backward()时，如果y是标量，则不需要为backward()传入任何参数。否则，需要传入一个与y同形的Tensor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa28935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFunction是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图(DAG), 每个Tensor都有一个.grad_fn属性，\\n该属性即创建该Tensor的Function，就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算有关的对象，否则是None\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function是另外一个很重要的类。Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图(DAG), 每个Tensor都有一个.grad_fn属性，\n",
    "该属性即创建该Tensor的Function，就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算有关的对象，否则是None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5af08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 创建一个Tensor并设置requires_grad=True\n",
    "\n",
    "x = torch.ones(2,2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca17890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000018F78E00A30>\n"
     ]
    }
   ],
   "source": [
    "# 做一下运算操作\n",
    "# 注意：x是直接创建的，所以没有grad_fn，而y是通过一个加法操作创建的，所以存在一个grad_fn\n",
    "y = x+2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896aca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MulBackward0 object at 0x0000018F78DC9840>\n",
      "<MeanBackward0 object at 0x0000018F78E00A60>\n"
     ]
    }
   ],
   "source": [
    "# 再做点复杂的运算操作\n",
    "z = y*y*3\n",
    "print(z)\n",
    "out=z.mean()\n",
    "print(out)\n",
    "print(z.grad_fn)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceebd70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7648, 1.9989],\n",
      "        [0.5056, 0.7952]])\n",
      "tensor([[ -9.7574,   6.0034],\n",
      "        [ -3.0685, -11.6477]])\n",
      "False\n",
      "True\n",
      "tensor(276.3333, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000018F7A121660>\n"
     ]
    }
   ],
   "source": [
    "# 通过.requires_grad_()来用in-place的方式改变requires_grad属性：\n",
    "\"\"\"\n",
    "默认情况下默认requires_grad=False, 这意味着pytorch不会跟踪a的计算历史，因此无法对其进行自动求导\n",
    "代码对a进行了一些计算，然后计算了b=(a*a).sum()，由于a的requires_grad属性为False,所以b的grad_fn属性为None,这意味着b不是通过可导运算得到的，\n",
    "因此无法对其进行自动求导\n",
    "\"\"\"\n",
    "a = torch.randn(2,2) # 默认情况下默认requires_grad=False\n",
    "print(a)\n",
    "a = ((a * 3) / (a -1))\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad=True\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26483ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0533, -1.4446],\n",
      "        [ 1.0433,  0.9986]])\n",
      "tensor([[ 1.5185e-01,  1.7728e+00],\n",
      "        [ 7.2337e+01, -2.1336e+03]])\n",
      "False\n",
      "tensor(4557441.5000)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2) # 默认情况下默认requires_grad=False\n",
    "print(a)\n",
    "a = ((a * 3) / (a -1))\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25de9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c02d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673475d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b82a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
