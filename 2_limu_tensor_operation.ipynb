{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6368337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281af16",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df31e969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7099, 0.8021, 0.2855],\n",
       "        [0.8545, 0.2541, 0.9079],\n",
       "        [0.1049, 0.3738, 0.4905],\n",
       "        [0.1984, 0.5105, 0.4735],\n",
       "        [0.6984, 0.6122, 0.4374]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7116b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6524, 0.3789, 0.4101],\n",
       "        [0.9763, 0.5198, 0.3374],\n",
       "        [0.2961, 0.3501, 0.9418],\n",
       "        [0.6093, 0.7544, 0.5625],\n",
       "        [0.4283, 0.1638, 0.6463]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c885e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1 加法形式一: +\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7386ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 加法形式二: .add()\n",
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee5dc272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 加法形式三: inplace\n",
    "# adds x to y\n",
    "# 注：PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "134320aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7099, 0.8021, 0.2855])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 索引\n",
    "\"\"\"\n",
    "可以使用类似Numpy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，\n",
    "也即修改一个，另一个会跟着修改\n",
    "\"\"\"\n",
    "z = x[0, :]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b468122",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0674, 1.6330, 1.3551])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z += 1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a579ebb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0674, 1.6330, 1.3551])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 源tensor也被修改了\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9d2b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function index_select in module torch:\n",
      "\n",
      "index_select(...)\n",
      "    index_select(input, dim, index, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new tensor which indexes the :attr:`input` tensor along dimension\n",
      "    :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`.\n",
      "    \n",
      "    The returned tensor has the same number of dimensions as the original tensor\n",
      "    (:attr:`input`).  The :attr:`dim`\\ th dimension has the same size as the length\n",
      "    of :attr:`index`; other dimensions have the same size as in the original tensor.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage as the original\n",
      "              tensor.  If :attr:`out` has a different shape than expected, we\n",
      "              silently change it to the correct shape, reallocating the underlying\n",
      "              storage if necessary.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim (int): the dimension in which we index\n",
      "        index (IntTensor or LongTensor): the 1-D tensor containing the indices to index\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-0.4664,  0.2647, -0.1228, -1.1068],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> indices = torch.tensor([0, 2])\n",
      "        >>> torch.index_select(x, 0, indices)\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> torch.index_select(x, 1, indices)\n",
      "        tensor([[ 0.1427, -0.5414],\n",
      "                [-0.4664, -0.1228],\n",
      "                [-1.1734,  0.7230]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. torch.index_select(input, dim, index), 第二个参数表示从第几维挑选数据，类型为int值；\n",
    "help(torch.index_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11870034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function masked_select in module torch:\n",
      "\n",
      "masked_select(...)\n",
      "    masked_select(input, mask, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new 1-D tensor which indexes the :attr:`input` tensor according to\n",
      "    the boolean mask :attr:`mask` which is a `BoolTensor`.\n",
      "    \n",
      "    The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need\n",
      "    to match, but they must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage\n",
      "              as the original tensor\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        mask  (BoolTensor): the tensor containing the binary mask to index with\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n",
      "                [-1.2035,  1.2252,  0.5002,  0.6248],\n",
      "                [ 0.1307, -2.0608,  0.1244,  2.0139]])\n",
      "        >>> mask = x.ge(0.5)\n",
      "        >>> mask\n",
      "        tensor([[False, False, False, False],\n",
      "                [False, True, True, True],\n",
      "                [False, False, False, True]])\n",
      "        >>> torch.masked_select(x, mask)\n",
      "        tensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. torch.masked_select(input, mask)\n",
    "help(torch.masked_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983c91dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1128,  0.7819, -0.0697, -1.7305],\n",
      "        [ 0.3671, -0.9227, -1.1336, -0.0990],\n",
      "        [ 0.3498, -0.5823, -0.7950,  0.6583]])\n",
      "tensor([[False,  True, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7819, 0.6583])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "print(mask)\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7c01464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function nonzero in module torch:\n",
      "\n",
      "nonzero(...)\n",
      "    nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors\n",
      "    \n",
      "    .. note::\n",
      "        :func:`torch.nonzero(..., as_tuple=False) <torch.nonzero>` (default) returns a\n",
      "        2-D tensor where each row is the index for a nonzero value.\n",
      "    \n",
      "        :func:`torch.nonzero(..., as_tuple=True) <torch.nonzero>` returns a tuple of 1-D\n",
      "        index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]``\n",
      "        gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor\n",
      "        contains nonzero indices for a certain dimension.\n",
      "    \n",
      "        See below for more details on the two behaviors.\n",
      "    \n",
      "        When :attr:`input` is on CUDA, :func:`torch.nonzero() <torch.nonzero>` causes\n",
      "        host-device synchronization.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``False`` **(default)**:\n",
      "    \n",
      "    Returns a tensor containing the indices of all non-zero elements of\n",
      "    :attr:`input`.  Each row in the result contains the indices of a non-zero\n",
      "    element in :attr:`input`. The result is sorted lexicographically, with\n",
      "    the last index changing the fastest (C-style).\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor\n",
      "    :attr:`out` is of size :math:`(z \\times n)`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``True``:\n",
      "    \n",
      "    Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`,\n",
      "    each containing the indices (in that dimension) of all non-zero elements of\n",
      "    :attr:`input` .\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n`\n",
      "    tensors of size :math:`z`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    As a special case, when :attr:`input` has zero dimensions and a nonzero scalar\n",
      "    value, it is treated as a one-dimensional tensor with one element.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (LongTensor, optional): the output tensor containing indices\n",
      "    \n",
      "    Returns:\n",
      "        LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output\n",
      "        tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for\n",
      "        each dimension, containing the indices of each nonzero element along that\n",
      "        dimension.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
      "        tensor([[ 0],\n",
      "                [ 1],\n",
      "                [ 2],\n",
      "                [ 4]])\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]))\n",
      "        tensor([[ 0,  0],\n",
      "                [ 1,  1],\n",
      "                [ 2,  2],\n",
      "                [ 3,  3]])\n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 4]),)\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
      "        >>> torch.nonzero(torch.tensor(5), as_tuple=True)\n",
      "        (tensor([0]),)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. torch.nonzero(input, as_tuple)\n",
    "# https://blog.csdn.net/wangxuecheng666/article/details/120639138\n",
    "# shape为Z*N（Z是非0的数的个数，N为input的维数）\n",
    "help(torch.nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e42b9837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [0, 3, 3]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 返回非0数对应的index\n",
    "x = torch.nonzero(torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]]))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9ecc5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6000,  0.4000,  0.5000,  1.2000, -0.4000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2返回多维innput中非零的数\n",
    "x = torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]])\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d512a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f92899",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3返回一维innput中非零的数\n",
    "x = torch.tensor([5,3])\n",
    "print(x)\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "471a8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function gather in module torch:\n",
      "\n",
      "gather(...)\n",
      "    gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor\n",
      "    \n",
      "    Gathers values along an axis specified by `dim`.\n",
      "    \n",
      "    For a 3-D tensor the output is specified by::\n",
      "    \n",
      "        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
      "        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
      "        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
      "    \n",
      "    :attr:`input` and :attr:`index` must have the same number of dimensions.\n",
      "    It is also required that ``index.size(d) <= input.size(d)`` for all\n",
      "    dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.\n",
      "    Note that ``input`` and ``index`` do not broadcast against each other.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the source tensor\n",
      "        dim (int): the axis along which to index\n",
      "        index (LongTensor): the indices of elements to gather\n",
      "    \n",
      "    Keyword arguments:\n",
      "        sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.\n",
      "        out (Tensor, optional): the destination tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> t = torch.tensor([[1, 2], [3, 4]])\n",
      "        >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\n",
      "        tensor([[ 1,  1],\n",
      "                [ 4,  3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. torch.gather(input, dim, index)\n",
    "# https://blog.csdn.net/Apikaqiu/article/details/104253080\n",
    "help(torch.gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb1f3a5f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4190fa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 0, torch.tensor([[0, 0], [1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf53ee22",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(a, 0,torch.tensor([8,4,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da92d43",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17],\n",
       "        [18, 19, 20, 21, 22, 23],\n",
       "        [24, 25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34, 35]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(36).reshape(6,6)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d20458",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 19, 26],\n",
       "        [ 0,  7, 14]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(b, 0, torch.tensor([[0,3,4],\n",
    "                                [0,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18fa7ac0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 4],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[[0, 3, 4],\n",
    "[6, 7,8]]\n",
    "\"\"\"\n",
    "torch.gather(b, 1, torch.tensor([[0,3,4],\n",
    "                                [0,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d7d6376",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10, 11],\n",
       "         [12, 13, 14, 15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20, 21, 22, 23],\n",
       "         [24, 25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34, 35]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.arange(36).reshape(2,3,6)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "720c729d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 0,  7, 14]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(c,1, torch.tensor([[[0,0,0],\n",
    "                   [0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e14698ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 0,  7, 14]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [18, 25, 32]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(c,1, torch.tensor([[[0,0,0],\n",
    "                   [0,1,2]],\n",
    "                               [[0,0,0],\n",
    "                   [0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0392556a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [0, 3, 3]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42dbf465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e6a9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 用view()来改变Tensor的形状\n",
    "\"\"\"\n",
    "注意view()返回的新tensor与源Tensor虽然可能有不同的size,但是共享data，也即是更改其中的一个，另外一个也会跟着改变。\n",
    "（顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变）\n",
    "\"\"\"\n",
    "y = x.view(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "266d69f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21e5d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.view(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c26225d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec592a7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9365082",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa06b5ee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "254359a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n",
      "tensor([[-1,  0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12, 13]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "# 8. clone().view()\n",
    "\"\"\"\n",
    "如果我们想返回一个真正的副本（即不共享data内存）该怎么办呢？---clone().view()\n",
    "pytorch还提供了一个reshape()苦役改变形状，但是此函数并不能保证返回的是其拷贝，\n",
    "所以不推荐使用.\n",
    "推荐先用clone创造一个副本然后再使用view\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "\"\"\"\n",
    "print(z)\n",
    "z_cp = z.clone().view(15)\n",
    "print(z_cp)\n",
    "z -= 1\n",
    "print(z)\n",
    "print(z_cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8ec1e47",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9166])\n",
      "1.9165549278259277\n"
     ]
    }
   ],
   "source": [
    "# 9. item()\n",
    "\"\"\"\n",
    "可以将一个标量转换成一个python number\n",
    "\"\"\"\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "492e1178",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6424, -1.8490,  0.2794],\n",
       "        [ 0.8115, -0.0530, -0.6954]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = torch.randn(2,3)\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804270b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10. #######线性代数相关的一些函数##############\n",
    "\"\"\"\n",
    "trace   对角线元素之和(矩阵的迹)\n",
    "diag  对角线元素\n",
    "triu/tril 矩阵的上三角/下三角，可指定偏移量\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "705c8192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function trace in module torch:\n",
      "\n",
      "trace(...)\n",
      "    trace(input) -> Tensor\n",
      "    \n",
      "    Returns the sum of the elements of the diagonal of the input 2-D matrix.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.arange(1., 10.).view(3, 3)\n",
      "        >>> x\n",
      "        tensor([[ 1.,  2.,  3.],\n",
      "                [ 4.,  5.,  6.],\n",
      "                [ 7.,  8.,  9.]])\n",
      "        >>> torch.trace(x)\n",
      "        tensor(15.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe1acfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function diag in module torch:\n",
      "\n",
      "diag(...)\n",
      "    diag(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor\n",
      "      with the elements of :attr:`input` as the diagonal.\n",
      "    - If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with\n",
      "      the diagonal elements of :attr:`input`.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider:\n",
      "    \n",
      "    - If :attr:`diagonal` = 0, it is the main diagonal.\n",
      "    - If :attr:`diagonal` > 0, it is above the main diagonal.\n",
      "    - If :attr:`diagonal` < 0, it is below the main diagonal.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "            :func:`torch.diagonal` always returns the diagonal of its input.\n",
      "    \n",
      "            :func:`torch.diagflat` always constructs a tensor with diagonal elements\n",
      "            specified by the input.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    Get the square matrix where the input vector is the diagonal::\n",
      "    \n",
      "        >>> a = torch.randn(3)\n",
      "        >>> a\n",
      "        tensor([ 0.5950,-0.0872, 2.3298])\n",
      "        >>> torch.diag(a)\n",
      "        tensor([[ 0.5950, 0.0000, 0.0000],\n",
      "                [ 0.0000,-0.0872, 0.0000],\n",
      "                [ 0.0000, 0.0000, 2.3298]])\n",
      "        >>> torch.diag(a, 1)\n",
      "        tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n",
      "                [ 0.0000, 0.0000,-0.0872, 0.0000],\n",
      "                [ 0.0000, 0.0000, 0.0000, 2.3298],\n",
      "                [ 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "    \n",
      "    Get the k-th diagonal of a given matrix::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[-0.4264, 0.0255,-0.1064],\n",
      "                [ 0.8795,-0.2429, 0.1374],\n",
      "                [ 0.1029,-0.6482,-1.6300]])\n",
      "        >>> torch.diag(a, 0)\n",
      "        tensor([-0.4264,-0.2429,-1.6300])\n",
      "        >>> torch.diag(a, 1)\n",
      "        tensor([ 0.0255, 0.1374])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input是一维，output就是二维， input是二维，则output是一维\n",
    "\"\"\"\n",
    "help(torch.diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d39f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function triu in module torch:\n",
      "\n",
      "triu(...)\n",
      "    triu(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices\n",
      "    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.\n",
      "    \n",
      "    The upper triangular part of the matrix is defined as the elements on and\n",
      "    above the diagonal.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider. If\n",
      "    :attr:`diagonal` = 0, all elements on and above the main diagonal are\n",
      "    retained. A positive value excludes just as many diagonals above the main\n",
      "    diagonal, and similarly a negative value includes just as many diagonals below\n",
      "    the main diagonal. The main diagonal are the set of indices\n",
      "    :math:`\\lbrace (i, i) \\rbrace` for :math:`i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]` where\n",
      "    :math:`d_{1}, d_{2}` are the dimensions of the matrix.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.2072, -1.0680,  0.6602],\n",
      "                [ 0.3480, -0.5211, -0.4573]])\n",
      "        >>> torch.triu(a)\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.0000, -1.0680,  0.6602],\n",
      "                [ 0.0000,  0.0000, -0.4573]])\n",
      "        >>> torch.triu(a, diagonal=1)\n",
      "        tensor([[ 0.0000,  0.5207,  2.0049],\n",
      "                [ 0.0000,  0.0000,  0.6602],\n",
      "                [ 0.0000,  0.0000,  0.0000]])\n",
      "        >>> torch.triu(a, diagonal=-1)\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.2072, -1.0680,  0.6602],\n",
      "                [ 0.0000, -0.5211, -0.4573]])\n",
      "    \n",
      "        >>> b = torch.randn(4, 6)\n",
      "        >>> b\n",
      "        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n",
      "                [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n",
      "        >>> torch.triu(b, diagonal=1)\n",
      "        tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n",
      "                [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n",
      "        >>> torch.triu(b, diagonal=-1)\n",
      "        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n",
      "                [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "991fb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function tril in module torch:\n",
      "\n",
      "tril(...)\n",
      "    tril(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices\n",
      "    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.\n",
      "    \n",
      "    The lower triangular part of the matrix is defined as the elements on and\n",
      "    below the diagonal.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider. If\n",
      "    :attr:`diagonal` = 0, all elements on and below the main diagonal are\n",
      "    retained. A positive value includes just as many diagonals above the main\n",
      "    diagonal, and similarly a negative value excludes just as many diagonals below\n",
      "    the main diagonal. The main diagonal are the set of indices\n",
      "    :math:`\\lbrace (i, i) \\rbrace` for :math:`i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]` where\n",
      "    :math:`d_{1}, d_{2}` are the dimensions of the matrix.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[-1.0813, -0.8619,  0.7105],\n",
      "                [ 0.0935,  0.1380,  2.2112],\n",
      "                [-0.3409, -0.9828,  0.0289]])\n",
      "        >>> torch.tril(a)\n",
      "        tensor([[-1.0813,  0.0000,  0.0000],\n",
      "                [ 0.0935,  0.1380,  0.0000],\n",
      "                [-0.3409, -0.9828,  0.0289]])\n",
      "    \n",
      "        >>> b = torch.randn(4, 6)\n",
      "        >>> b\n",
      "        tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n",
      "                [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n",
      "                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n",
      "                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n",
      "        >>> torch.tril(b, diagonal=1)\n",
      "        tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n",
      "                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n",
      "                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n",
      "        >>> torch.tril(b, diagonal=-1)\n",
      "        tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3315d34f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mm in module torch:\n",
      "\n",
      "mm(...)\n",
      "    mm(input, mat2, *, out=None) -> Tensor\n",
      "    \n",
      "    Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.\n",
      "    \n",
      "    If :attr:`input` is a :math:`(n \\times m)` tensor, :attr:`mat2` is a\n",
      "    :math:`(m \\times p)` tensor, :attr:`out` will be a :math:`(n \\times p)` tensor.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "    \n",
      "    Supports strided and sparse 2-D tensors as inputs, autograd with\n",
      "    respect to strided inputs.\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the first matrix to be matrix multiplied\n",
      "        mat2 (Tensor): the second matrix to be matrix multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> mat1 = torch.randn(2, 3)\n",
      "        >>> mat2 = torch.randn(3, 3)\n",
      "        >>> torch.mm(mat1, mat2)\n",
      "        tensor([[ 0.4851,  0.5037, -0.3633],\n",
      "                [-0.0760, -3.6705,  2.4784]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n",
    "# 数学上的矩阵相乘\n",
    "help(torch.mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50ba919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2621,  2.3984, -1.6417],\n",
       "        [ 0.9421,  0.0194, -0.1172]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71bee1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3501, -0.9045],\n",
       "        [ 1.0299, -0.0246],\n",
       "        [-0.8473,  0.7765]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = torch.randn(3, 2)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b0db9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2151, -1.5709],\n",
       "        [ 1.3911, -0.9436]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d2261da",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.256522970000001"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5752*1.6299 + 1.2822*2.9772 -0.6495*0.7673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b3a4e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function bmm in module torch:\n",
      "\n",
      "bmm(...)\n",
      "    bmm(input, mat2, *, out=None) -> Tensor\n",
      "    \n",
      "    Performs a batch matrix-matrix product of matrices stored in :attr:`input`\n",
      "    and :attr:`mat2`.\n",
      "    \n",
      "    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing\n",
      "    the same number of matrices.\n",
      "    \n",
      "    If :attr:`input` is a :math:`(b \\times n \\times m)` tensor, :attr:`mat2` is a\n",
      "    :math:`(b \\times m \\times p)` tensor, :attr:`out` will be a\n",
      "    :math:`(b \\times n \\times p)` tensor.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the first batch of matrices to be multiplied\n",
      "        mat2 (Tensor): the second batch of matrices to be multiplied\n",
      "    \n",
      "    Keyword Args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> input = torch.randn(10, 3, 4)\n",
      "        >>> mat2 = torch.randn(10, 4, 5)\n",
      "        >>> res = torch.bmm(input, mat2)\n",
      "        >>> res.size()\n",
      "        torch.Size([10, 3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n",
    "help(torch.bmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0face7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[[-1.1847,  0.0410, -0.5768,  0.4169],\n",
      "         [-0.6415, -0.3177, -0.8972, -0.7950],\n",
      "         [-2.9986,  2.6437, -1.8537,  0.4957]],\n",
      "\n",
      "        [[-1.2137,  2.3454, -0.1044, -0.6831],\n",
      "         [ 0.3544,  0.2728,  0.8918,  0.5072],\n",
      "         [ 0.0171, -1.3940,  1.0164, -0.0145]]])\n",
      "mat2:tensor([[[ 0.6089,  0.5453, -0.9820,  0.2671, -1.3822],\n",
      "         [ 0.2006,  0.2895, -0.9884,  2.1604,  2.5285],\n",
      "         [-1.4788, -2.0407,  0.2791, -0.0694, -0.5121],\n",
      "         [-0.3281, -1.0097, -0.3885,  0.8986,  0.5167]],\n",
      "\n",
      "        [[-1.3075, -0.0859,  0.2601,  0.7127, -0.1840],\n",
      "         [-0.2685, -0.1976, -1.5441, -1.3401,  0.8365],\n",
      "         [-0.8632, -0.5366,  0.3598, -0.3461, -1.2170],\n",
      "         [-0.5274, -1.4887,  0.3305, -0.3523,  1.0441]]])\n",
      "res:tensor([[[ 3.1184e-03,  1.2203e-01,  7.9981e-01,  1.8691e-01,  2.2520e+00],\n",
      "         [ 1.1333e+00,  2.1919e+00,  1.0024e+00, -1.5099e+00,  1.3191e-01],\n",
      "         [ 1.2830e+00,  2.4125e+00, -3.7852e-01,  5.4848e+00,  1.2035e+01]],\n",
      "\n",
      "        [[ 1.4076e+00,  7.1383e-01, -4.2006e+00, -3.7312e+00,  1.5991e+00],\n",
      "         [-1.5739e+00, -1.3180e+00,  1.5947e-01, -6.0040e-01, -3.9279e-01],\n",
      "         [-5.1782e-01, -2.4990e-01,  2.5179e+00,  1.5336e+00, -2.4213e+00]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3, 4)\n",
    "print(f\"input:{input}\")\n",
    "mat2 = torch.randn(2, 4, 5)\n",
    "print(f\"mat2:{mat2}\")\n",
    "res = torch.bmm(input, mat2)\n",
    "print(f\"res:{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de517c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030477199999999205"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.1847) *0.6089  + 0.0410*0.2006 + (-0.5768)*(-1.4788) + 0.4169*(-0.3281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fde863f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function addmm in module torch:\n",
      "\n",
      "addmm(...)\n",
      "    addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor\n",
      "    \n",
      "    Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.\n",
      "    The matrix :attr:`input` is added to the final result.\n",
      "    \n",
      "    If :attr:`mat1` is a :math:`(n \\times m)` tensor, :attr:`mat2` is a\n",
      "    :math:`(m \\times p)` tensor, then :attr:`input` must be\n",
      "    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \\times p)` tensor\n",
      "    and :attr:`out` will be a :math:`(n \\times p)` tensor.\n",
      "    \n",
      "    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between\n",
      "    :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i)\n",
      "    \n",
      "    If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in\n",
      "    it will not be propagated.\n",
      "    \n",
      "    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and\n",
      "    :attr:`alpha` must be real numbers, otherwise they should be integers.\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): matrix to be added\n",
      "        mat1 (Tensor): the first matrix to be matrix multiplied\n",
      "        mat2 (Tensor): the second matrix to be matrix multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        beta (Number, optional): multiplier for :attr:`input` (:math:`\\beta`)\n",
      "        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\\alpha`)\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> M = torch.randn(2, 3)\n",
      "        >>> mat1 = torch.randn(2, 3)\n",
      "        >>> mat2 = torch.randn(3, 3)\n",
      "        >>> torch.addmm(M, mat1, mat2)\n",
      "        tensor([[-4.8716,  1.4671, -1.3746],\n",
      "                [ 0.7573, -3.9555, -2.8681]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.addmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc4f4b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3381, -0.6834,  0.4693],\n",
       "        [ 0.7009, -0.3207, -0.5713]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.randn(2, 3)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90c77ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5324, -0.3905,  0.4314],\n",
       "        [-1.7555, -0.1326, -0.9685]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0c7f0075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3579,  0.0051,  0.7342],\n",
       "        [ 0.2459,  2.9872, -0.5393],\n",
       "        [ 0.2490,  1.2793,  1.5052]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = torch.randn(3, 3)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dfdf2907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5172, -1.3008,  0.9384],\n",
       "        [-0.2012, -1.9649, -3.2465]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(M, mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d959d9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.51725131"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.5324)*0.3579 + (-0.3905)*0.2459 + 0.4314*0.2490+(-0.3381)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7db006be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function addbmm in module torch:\n",
      "\n",
      "addbmm(...)\n",
      "    addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor\n",
      "    \n",
      "    Performs a batch matrix-matrix product of matrices stored\n",
      "    in :attr:`batch1` and :attr:`batch2`,\n",
      "    with a reduced add step (all matrix multiplications get accumulated\n",
      "    along the first dimension).\n",
      "    :attr:`input` is added to the final result.\n",
      "    \n",
      "    :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the\n",
      "    same number of matrices.\n",
      "    \n",
      "    If :attr:`batch1` is a :math:`(b \\times n \\times m)` tensor, :attr:`batch2` is a\n",
      "    :math:`(b \\times m \\times p)` tensor, :attr:`input` must be\n",
      "    :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \\times p)` tensor\n",
      "    and :attr:`out` will be a :math:`(n \\times p)` tensor.\n",
      "    \n",
      "    .. math::\n",
      "        out = \\beta\\ \\text{input} + \\alpha\\ (\\sum_{i=0}^{b-1} \\text{batch1}_i \\mathbin{@} \\text{batch2}_i)\n",
      "    \n",
      "    If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in\n",
      "    it will not be propagated.\n",
      "    \n",
      "    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`\n",
      "    must be real numbers, otherwise they should be integers.\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    Args:\n",
      "        batch1 (Tensor): the first batch of matrices to be multiplied\n",
      "        batch2 (Tensor): the second batch of matrices to be multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        beta (Number, optional): multiplier for :attr:`input` (:math:`\\beta`)\n",
      "        input (Tensor): matrix to be added\n",
      "        alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\\alpha`)\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> M = torch.randn(3, 5)\n",
      "        >>> batch1 = torch.randn(10, 3, 4)\n",
      "        >>> batch2 = torch.randn(10, 4, 5)\n",
      "        >>> torch.addbmm(M, batch1, batch2)\n",
      "        tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n",
      "                [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n",
      "                [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.addbmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17b5a8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "batch1:tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "batch2:tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5.],\n",
       "        [5., 5.],\n",
       "        [5., 5.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addbmm(input, batch1, batch2)会将batch1和batch2乘起来之后然后相加，之后再和input相加起来\n",
    "\n",
    "M = torch.ones(3, 2)\n",
    "print(f\"M:{M}\")\n",
    "batch1 = torch.ones(2, 3, 2)\n",
    "print(f\"batch1:{batch1}\")\n",
    "batch2 = torch.ones(2, 2, 2)\n",
    "print(f\"batch2:{batch2}\")\n",
    "torch.addbmm(M, batch1, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b7f2e8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(batch1, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b1fe6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function addmv in module torch:\n",
      "\n",
      "addmv(...)\n",
      "    addmv(input, mat, vec, *, beta=1, alpha=1, out=None) -> Tensor\n",
      "    \n",
      "    Performs a matrix-vector product of the matrix :attr:`mat` and\n",
      "    the vector :attr:`vec`.\n",
      "    The vector :attr:`input` is added to the final result.\n",
      "    \n",
      "    If :attr:`mat` is a :math:`(n \\times m)` tensor, :attr:`vec` is a 1-D tensor of\n",
      "    size `m`, then :attr:`input` must be\n",
      "    :ref:`broadcastable <broadcasting-semantics>` with a 1-D tensor of size `n` and\n",
      "    :attr:`out` will be 1-D tensor of size `n`.\n",
      "    \n",
      "    :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between\n",
      "    :attr:`mat` and :attr:`vec` and the added tensor :attr:`input` respectively.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat} \\mathbin{@} \\text{vec})\n",
      "    \n",
      "    If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in\n",
      "    it will not be propagated.\n",
      "    \n",
      "    For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and\n",
      "    :attr:`alpha` must be real numbers, otherwise they should be integers\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): vector to be added\n",
      "        mat (Tensor): matrix to be matrix multiplied\n",
      "        vec (Tensor): vector to be matrix multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        beta (Number, optional): multiplier for :attr:`input` (:math:`\\beta`)\n",
      "        alpha (Number, optional): multiplier for :math:`mat @ vec` (:math:`\\alpha`)\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> M = torch.randn(2)\n",
      "        >>> mat = torch.randn(2, 3)\n",
      "        >>> vec = torch.randn(3)\n",
      "        >>> torch.addmv(M, mat, vec)\n",
      "        tensor([-0.3768, -5.5565])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.addmv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "add92bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:tensor([-0.3752, -0.6982])\n",
      "mat:tensor([[ 1.8442,  0.9186,  0.6109],\n",
      "        [-2.8610, -0.9939,  0.7575]])\n",
      "vec:tensor([-0.9267, -1.0354, -1.0850])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-3.6983,  2.1605])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = torch.randn(2)\n",
    "print(f\"M:{M}\")\n",
    "mat  = torch.randn(2, 3)\n",
    "print(f\"mat:{mat}\")\n",
    "vec = torch.randn(3)\n",
    "print(f\"vec:{vec}\")\n",
    "torch.addmv(M, mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6df9a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6983,  2.1604])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(mat, vec) + torch.tensor([-0.3752, -0.6982])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e2dc0229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mv in module torch:\n",
      "\n",
      "mv(...)\n",
      "    mv(input, vec, *, out=None) -> Tensor\n",
      "    \n",
      "    Performs a matrix-vector product of the matrix :attr:`input` and the vector\n",
      "    :attr:`vec`.\n",
      "    \n",
      "    If :attr:`input` is a :math:`(n \\times m)` tensor, :attr:`vec` is a 1-D tensor of\n",
      "    size :math:`m`, :attr:`out` will be 1-D of size :math:`n`.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): matrix to be multiplied\n",
      "        vec (Tensor): vector to be multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> mat = torch.randn(2, 3)\n",
      "        >>> vec = torch.randn(3)\n",
      "        >>> torch.mv(mat, vec)\n",
      "        tensor([ 1.0404, -0.6361])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf823975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat:tensor([[ 1.0031,  0.5251,  0.2597],\n",
      "        [ 0.4389, -1.0294, -1.1313]])\n",
      "vec:tensor([-0.2550, -1.1652,  0.2843])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.7938,  0.7659])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://blog.csdn.net/qq_41845478/article/details/107490718\n",
    "# mat*(vec.t)\n",
    "import torch\n",
    "mat  = torch.randn(2, 3)\n",
    "print(f\"mat:{mat}\")\n",
    "vec = torch.randn(3)\n",
    "print(f\"vec:{vec}\")\n",
    "torch.mv( mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6763cd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7938043100000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.2550)*1.0031 + (-1.1652)*0.5251+(0.2597)*(0.2843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e014a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7659087900000001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.2550)*0.4389 + (-1.1652)*(-1.0294)+(-1.1313)*(0.2843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ab80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function addr in module torch:\n",
      "\n",
      "addr(...)\n",
      "    addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) -> Tensor\n",
      "    \n",
      "    Performs the outer-product of vectors :attr:`vec1` and :attr:`vec2`\n",
      "    and adds it to the matrix :attr:`input`.\n",
      "    \n",
      "    Optional values :attr:`beta` and :attr:`alpha` are scaling factors on the\n",
      "    outer product between :attr:`vec1` and :attr:`vec2` and the added matrix\n",
      "    :attr:`input` respectively.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{vec1} \\otimes \\text{vec2})\n",
      "    \n",
      "    If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in\n",
      "    it will not be propagated.\n",
      "    \n",
      "    If :attr:`vec1` is a vector of size `n` and :attr:`vec2` is a vector\n",
      "    of size `m`, then :attr:`input` must be\n",
      "    :ref:`broadcastable <broadcasting-semantics>` with a matrix of size\n",
      "    :math:`(n \\times m)` and :attr:`out` will be a matrix of size\n",
      "    :math:`(n \\times m)`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): matrix to be added\n",
      "        vec1 (Tensor): the first vector of the outer product\n",
      "        vec2 (Tensor): the second vector of the outer product\n",
      "    \n",
      "    Keyword args:\n",
      "        beta (Number, optional): multiplier for :attr:`input` (:math:`\\beta`)\n",
      "        alpha (Number, optional): multiplier for :math:`\\text{vec1} \\otimes \\text{vec2}` (:math:`\\alpha`)\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> vec1 = torch.arange(1., 4.)\n",
      "        >>> vec2 = torch.arange(1., 3.)\n",
      "        >>> M = torch.zeros(3, 2)\n",
      "        >>> torch.addr(M, vec1, vec2)\n",
      "        tensor([[ 1.,  2.],\n",
      "                [ 2.,  4.],\n",
      "                [ 3.,  6.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff952fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'r'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m help(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'r'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8adcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97087b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7d24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8e686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f386d171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. 用方法to()可以将Tensor在CPU和GPU(需要硬件支持)之间相互移动\n",
    "\"\"\"\n",
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n",
    "\"\"\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd4b8f1d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy=torch.ones_like(yy)\n",
    "yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8588cacc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = yyy.to(\"cpu\", torch.double)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7d97a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function dot in module torch:\n",
      "\n",
      "dot(...)\n",
      "    dot(input, other, *, out=None) -> Tensor\n",
      "    \n",
      "    Computes the dot product of two 1D tensors.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        Unlike NumPy's dot, torch.dot intentionally only supports computing the dot product\n",
      "        of two 1D tensors with the same number of elements.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): first tensor in the dot product, must be 1D.\n",
      "        other (Tensor): second tensor in the dot product, must be 1D.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\n",
      "        tensor(7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 内积：又称为点乘， 就是线性代数里面行和列对应元素相乘然后求和。 两向量的内积结果就是标量. torch.dot\n",
    "# 外积：两个向量的张量积，其结果是一个矩阵。torch.cross\n",
    "help(torch.dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7c03aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function cross in module torch:\n",
      "\n",
      "cross(...)\n",
      "    cross(input, other, dim=None, *, out=None) -> Tensor\n",
      "    \n",
      "    \n",
      "    Returns the cross product of vectors in dimension :attr:`dim` of :attr:`input`\n",
      "    and :attr:`other`.\n",
      "    \n",
      "    Supports input of float, double, cfloat and cdouble dtypes. Also supports batches\n",
      "    of vectors, for which it computes the product along the dimension :attr:`dim`.\n",
      "    In this case, the output has the same batch dimensions as the inputs.\n",
      "    \n",
      "    If :attr:`dim` is not given, it defaults to the first dimension found with the\n",
      "    size 3. Note that this might be unexpected.\n",
      "    \n",
      "    .. seealso::\n",
      "            :func:`torch.linalg.cross` which requires specifying dim (defaulting to -1).\n",
      "    \n",
      "    .. warning:: This function may change in a future PyTorch release to match\n",
      "            the default behaviour in :func:`torch.linalg.cross`. We recommend using\n",
      "            :func:`torch.linalg.cross`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        other (Tensor): the second input tensor\n",
      "        dim  (int, optional): the dimension to take the cross-product in.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(4, 3)\n",
      "        >>> a\n",
      "        tensor([[-0.3956,  1.1455,  1.6895],\n",
      "                [-0.5849,  1.3672,  0.3599],\n",
      "                [-1.1626,  0.7180, -0.0521],\n",
      "                [-0.1339,  0.9902, -2.0225]])\n",
      "        >>> b = torch.randn(4, 3)\n",
      "        >>> b\n",
      "        tensor([[-0.0257, -1.4725, -1.2251],\n",
      "                [-1.1479, -0.7005, -1.9757],\n",
      "                [-1.3904,  0.3726, -1.1836],\n",
      "                [-0.9688, -0.7153,  0.2159]])\n",
      "        >>> torch.cross(a, b, dim=1)\n",
      "        tensor([[ 1.0844, -0.5281,  0.6120],\n",
      "                [-2.4490, -1.5687,  1.9792],\n",
      "                [-0.8304, -1.3037,  0.5650],\n",
      "                [-1.2329,  1.9883,  1.0551]])\n",
      "        >>> torch.cross(a, b)\n",
      "        tensor([[ 1.0844, -0.5281,  0.6120],\n",
      "                [-2.4490, -1.5687,  1.9792],\n",
      "                [-0.8304, -1.3037,  0.5650],\n",
      "                [-1.2329,  1.9883,  1.0551]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300a94c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cross_matrix.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 结果中的每个值是怎么计算得来的，请举例计算出结果中的一个值\n",
    "from IPython.display import Image\n",
    "Image(url=\"cross_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd4b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0844367 -0.52806971 0.6119603499999999\n"
     ]
    }
   ],
   "source": [
    "cx = 1.1455 * (-1.2251) - 1.6895 * (-1.4725) # = 1.0844\n",
    "cy = 1.6895 * (-0.0257) - (-0.3956) * (-1.2251) # = -0.5281\n",
    "cz = (-0.3956) * (-1.4725) - 1.1455 * (-0.0257) # = 0.6120\n",
    "print(cx, cy, cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5887d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cross_formula.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"cross_formula.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4ab60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cross.png\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"cross.png\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92635e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3,  6, -3])\n"
     ]
    }
   ],
   "source": [
    "# 定义两个3D向量\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 计算叉积\n",
    "c = torch.cross(a, b)\n",
    "print(c)\n",
    "\n",
    "\"\"\"\n",
    "cx=12-15=-3\n",
    "cy=12-6=6\n",
    "cz=5-8=-3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e12e0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunya\\AppData\\Local\\Temp\\ipykernel_10872\\2533872927.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2985.)\n",
      "  a.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9309ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39d047a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3,  6, -3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cross(a.T, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2116b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function inverse in module torch:\n",
      "\n",
      "inverse(...)\n",
      "    inverse(input, *, out=None) -> Tensor\n",
      "    \n",
      "    Alias for :func:`torch.linalg.inv`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inverse 求逆矩阵\n",
    "help(torch.inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12d7e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function dot in module torch:\n",
      "\n",
      "dot(...)\n",
      "    dot(input, other, *, out=None) -> Tensor\n",
      "    \n",
      "    Computes the dot product of two 1D tensors.\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "        Unlike NumPy's dot, torch.dot intentionally only supports computing the dot product\n",
      "        of two 1D tensors with the same number of elements.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): first tensor in the dot product, must be 1D.\n",
      "        other (Tensor): second tensor in the dot product, must be 1D.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\n",
      "        tensor(7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23302882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
