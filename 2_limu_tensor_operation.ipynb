{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6368337f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281af16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df31e969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1547, 0.1591, 0.3363],\n",
       "        [0.7975, 0.2950, 0.7956],\n",
       "        [0.6639, 0.6565, 0.2288],\n",
       "        [0.9113, 0.1911, 0.8595],\n",
       "        [0.5151, 0.7153, 0.2668]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7116b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6502, 0.9777, 0.8858],\n",
       "        [0.4638, 0.4591, 0.9167],\n",
       "        [0.2230, 0.3068, 0.6521],\n",
       "        [0.6540, 0.1431, 0.6963],\n",
       "        [0.2428, 0.3387, 0.9601]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c885e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8048, 1.1368, 1.2221],\n",
       "        [1.2613, 0.7542, 1.7123],\n",
       "        [0.8869, 0.9633, 0.8809],\n",
       "        [1.5653, 0.3342, 1.5559],\n",
       "        [0.7579, 1.0541, 1.2269]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1 加法形式一: +\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7386ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8048, 1.1368, 1.2221],\n",
       "        [1.2613, 0.7542, 1.7123],\n",
       "        [0.8869, 0.9633, 0.8809],\n",
       "        [1.5653, 0.3342, 1.5559],\n",
       "        [0.7579, 1.0541, 1.2269]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 加法形式二: .add()\n",
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee5dc272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8048, 1.1368, 1.2221],\n",
       "        [1.2613, 0.7542, 1.7123],\n",
       "        [0.8869, 0.9633, 0.8809],\n",
       "        [1.5653, 0.3342, 1.5559],\n",
       "        [0.7579, 1.0541, 1.2269]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 加法形式三: inplace\n",
    "# adds x to y\n",
    "# 注：PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134320aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1547, 0.1591, 0.3363])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 索引\n",
    "\"\"\"\n",
    "可以使用类似Numpy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，\n",
    "也即修改一个，另一个会跟着修改\n",
    "\"\"\"\n",
    "z = x[0, :]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b468122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1547, 1.1591, 1.3363])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z += 1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a579ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1547, 1.1591, 1.3363])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 源tensor也被修改了\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9d2b1c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function index_select in module torch:\n",
      "\n",
      "index_select(...)\n",
      "    index_select(input, dim, index, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new tensor which indexes the :attr:`input` tensor along dimension\n",
      "    :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`.\n",
      "    \n",
      "    The returned tensor has the same number of dimensions as the original tensor\n",
      "    (:attr:`input`).  The :attr:`dim`\\ th dimension has the same size as the length\n",
      "    of :attr:`index`; other dimensions have the same size as in the original tensor.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage as the original\n",
      "              tensor.  If :attr:`out` has a different shape than expected, we\n",
      "              silently change it to the correct shape, reallocating the underlying\n",
      "              storage if necessary.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim (int): the dimension in which we index\n",
      "        index (IntTensor or LongTensor): the 1-D tensor containing the indices to index\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-0.4664,  0.2647, -0.1228, -1.1068],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> indices = torch.tensor([0, 2])\n",
      "        >>> torch.index_select(x, 0, indices)\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> torch.index_select(x, 1, indices)\n",
      "        tensor([[ 0.1427, -0.5414],\n",
      "                [-0.4664, -0.1228],\n",
      "                [-1.1734,  0.7230]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. torch.index_select(input, dim, index), 第二个参数表示从第几维挑选数据，类型为int值；\n",
    "help(torch.index_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11870034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function masked_select in module torch:\n",
      "\n",
      "masked_select(...)\n",
      "    masked_select(input, mask, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new 1-D tensor which indexes the :attr:`input` tensor according to\n",
      "    the boolean mask :attr:`mask` which is a `BoolTensor`.\n",
      "    \n",
      "    The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need\n",
      "    to match, but they must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage\n",
      "              as the original tensor\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        mask  (BoolTensor): the tensor containing the binary mask to index with\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n",
      "                [-1.2035,  1.2252,  0.5002,  0.6248],\n",
      "                [ 0.1307, -2.0608,  0.1244,  2.0139]])\n",
      "        >>> mask = x.ge(0.5)\n",
      "        >>> mask\n",
      "        tensor([[False, False, False, False],\n",
      "                [False, True, True, True],\n",
      "                [False, False, False, True]])\n",
      "        >>> torch.masked_select(x, mask)\n",
      "        tensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. torch.masked_select(input, mask)\n",
    "help(torch.masked_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "983c91dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9306, -0.4766,  1.0969,  0.4409],\n",
      "        [ 0.0736,  0.8487,  0.8701, -1.2438],\n",
      "        [-0.6976, -0.5362,  0.7883,  0.4097]])\n",
      "tensor([[False, False,  True, False],\n",
      "        [False,  True,  True, False],\n",
      "        [False, False,  True, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0969, 0.8487, 0.8701, 0.7883])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "print(mask)\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7c01464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function nonzero in module torch:\n",
      "\n",
      "nonzero(...)\n",
      "    nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors\n",
      "    \n",
      "    .. note::\n",
      "        :func:`torch.nonzero(..., as_tuple=False) <torch.nonzero>` (default) returns a\n",
      "        2-D tensor where each row is the index for a nonzero value.\n",
      "    \n",
      "        :func:`torch.nonzero(..., as_tuple=True) <torch.nonzero>` returns a tuple of 1-D\n",
      "        index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]``\n",
      "        gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor\n",
      "        contains nonzero indices for a certain dimension.\n",
      "    \n",
      "        See below for more details on the two behaviors.\n",
      "    \n",
      "        When :attr:`input` is on CUDA, :func:`torch.nonzero() <torch.nonzero>` causes\n",
      "        host-device synchronization.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``False`` **(default)**:\n",
      "    \n",
      "    Returns a tensor containing the indices of all non-zero elements of\n",
      "    :attr:`input`.  Each row in the result contains the indices of a non-zero\n",
      "    element in :attr:`input`. The result is sorted lexicographically, with\n",
      "    the last index changing the fastest (C-style).\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor\n",
      "    :attr:`out` is of size :math:`(z \\times n)`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``True``:\n",
      "    \n",
      "    Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`,\n",
      "    each containing the indices (in that dimension) of all non-zero elements of\n",
      "    :attr:`input` .\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n`\n",
      "    tensors of size :math:`z`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    As a special case, when :attr:`input` has zero dimensions and a nonzero scalar\n",
      "    value, it is treated as a one-dimensional tensor with one element.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (LongTensor, optional): the output tensor containing indices\n",
      "    \n",
      "    Returns:\n",
      "        LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output\n",
      "        tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for\n",
      "        each dimension, containing the indices of each nonzero element along that\n",
      "        dimension.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
      "        tensor([[ 0],\n",
      "                [ 1],\n",
      "                [ 2],\n",
      "                [ 4]])\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]))\n",
      "        tensor([[ 0,  0],\n",
      "                [ 1,  1],\n",
      "                [ 2,  2],\n",
      "                [ 3,  3]])\n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 4]),)\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
      "        >>> torch.nonzero(torch.tensor(5), as_tuple=True)\n",
      "        (tensor([0]),)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. torch.nonzero(input, as_tuple)\n",
    "# https://blog.csdn.net/wangxuecheng666/article/details/120639138\n",
    "# shape为Z*N（Z是非0的数的个数，N为input的维数）\n",
    "help(torch.nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e42b9837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [0, 3, 3]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 返回非0数对应的index\n",
    "x = torch.nonzero(torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]]))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b9ecc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6000,  0.4000,  0.5000,  1.2000, -0.4000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2返回多维innput中非零的数\n",
    "x = torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]])\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33f92899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3返回一维innput中非零的数\n",
    "x = torch.tensor([5,3])\n",
    "print(x)\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "471a8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function gather in module torch:\n",
      "\n",
      "gather(...)\n",
      "    gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor\n",
      "    \n",
      "    Gathers values along an axis specified by `dim`.\n",
      "    \n",
      "    For a 3-D tensor the output is specified by::\n",
      "    \n",
      "        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
      "        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
      "        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
      "    \n",
      "    :attr:`input` and :attr:`index` must have the same number of dimensions.\n",
      "    It is also required that ``index.size(d) <= input.size(d)`` for all\n",
      "    dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.\n",
      "    Note that ``input`` and ``index`` do not broadcast against each other.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the source tensor\n",
      "        dim (int): the axis along which to index\n",
      "        index (LongTensor): the indices of elements to gather\n",
      "    \n",
      "    Keyword arguments:\n",
      "        sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.\n",
      "        out (Tensor, optional): the destination tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> t = torch.tensor([[1, 2], [3, 4]])\n",
      "        >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\n",
      "        tensor([[ 1,  1],\n",
      "                [ 4,  3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. torch.gather(input, dim, index)\n",
    "# https://blog.csdn.net/Apikaqiu/article/details/104253080\n",
    "help(torch.gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
