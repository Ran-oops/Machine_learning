{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6368337f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281af16",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df31e969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7099, 0.8021, 0.2855],\n",
       "        [0.8545, 0.2541, 0.9079],\n",
       "        [0.1049, 0.3738, 0.4905],\n",
       "        [0.1984, 0.5105, 0.4735],\n",
       "        [0.6984, 0.6122, 0.4374]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7116b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6524, 0.3789, 0.4101],\n",
       "        [0.9763, 0.5198, 0.3374],\n",
       "        [0.2961, 0.3501, 0.9418],\n",
       "        [0.6093, 0.7544, 0.5625],\n",
       "        [0.4283, 0.1638, 0.6463]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c885e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1 加法形式一: +\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7386ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 加法形式二: .add()\n",
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee5dc272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3622, 1.1810, 0.6956],\n",
       "        [1.8308, 0.7739, 1.2453],\n",
       "        [0.4010, 0.7239, 1.4323],\n",
       "        [0.8077, 1.2650, 1.0360],\n",
       "        [1.1267, 0.7760, 1.0837]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 加法形式三: inplace\n",
    "# adds x to y\n",
    "# 注：PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "134320aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7099, 0.8021, 0.2855])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 索引\n",
    "\"\"\"\n",
    "可以使用类似Numpy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，\n",
    "也即修改一个，另一个会跟着修改\n",
    "\"\"\"\n",
    "z = x[0, :]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b468122",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0674, 1.6330, 1.3551])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z += 1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a579ebb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0674, 1.6330, 1.3551])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 源tensor也被修改了\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9d2b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function index_select in module torch:\n",
      "\n",
      "index_select(...)\n",
      "    index_select(input, dim, index, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new tensor which indexes the :attr:`input` tensor along dimension\n",
      "    :attr:`dim` using the entries in :attr:`index` which is a `LongTensor`.\n",
      "    \n",
      "    The returned tensor has the same number of dimensions as the original tensor\n",
      "    (:attr:`input`).  The :attr:`dim`\\ th dimension has the same size as the length\n",
      "    of :attr:`index`; other dimensions have the same size as in the original tensor.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage as the original\n",
      "              tensor.  If :attr:`out` has a different shape than expected, we\n",
      "              silently change it to the correct shape, reallocating the underlying\n",
      "              storage if necessary.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        dim (int): the dimension in which we index\n",
      "        index (IntTensor or LongTensor): the 1-D tensor containing the indices to index\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-0.4664,  0.2647, -0.1228, -1.1068],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> indices = torch.tensor([0, 2])\n",
      "        >>> torch.index_select(x, 0, indices)\n",
      "        tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n",
      "                [-1.1734, -0.6571,  0.7230, -0.6004]])\n",
      "        >>> torch.index_select(x, 1, indices)\n",
      "        tensor([[ 0.1427, -0.5414],\n",
      "                [-0.4664, -0.1228],\n",
      "                [-1.1734,  0.7230]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. torch.index_select(input, dim, index), 第二个参数表示从第几维挑选数据，类型为int值；\n",
    "help(torch.index_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11870034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function masked_select in module torch:\n",
      "\n",
      "masked_select(...)\n",
      "    masked_select(input, mask, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns a new 1-D tensor which indexes the :attr:`input` tensor according to\n",
      "    the boolean mask :attr:`mask` which is a `BoolTensor`.\n",
      "    \n",
      "    The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need\n",
      "    to match, but they must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      "    \n",
      "    .. note:: The returned tensor does **not** use the same storage\n",
      "              as the original tensor\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        mask  (BoolTensor): the tensor containing the binary mask to index with\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(3, 4)\n",
      "        >>> x\n",
      "        tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n",
      "                [-1.2035,  1.2252,  0.5002,  0.6248],\n",
      "                [ 0.1307, -2.0608,  0.1244,  2.0139]])\n",
      "        >>> mask = x.ge(0.5)\n",
      "        >>> mask\n",
      "        tensor([[False, False, False, False],\n",
      "                [False, True, True, True],\n",
      "                [False, False, False, True]])\n",
      "        >>> torch.masked_select(x, mask)\n",
      "        tensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. torch.masked_select(input, mask)\n",
    "help(torch.masked_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983c91dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1128,  0.7819, -0.0697, -1.7305],\n",
      "        [ 0.3671, -0.9227, -1.1336, -0.0990],\n",
      "        [ 0.3498, -0.5823, -0.7950,  0.6583]])\n",
      "tensor([[False,  True, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7819, 0.6583])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "print(x)\n",
    "mask = x.ge(0.5)\n",
    "print(mask)\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7c01464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function nonzero in module torch:\n",
      "\n",
      "nonzero(...)\n",
      "    nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors\n",
      "    \n",
      "    .. note::\n",
      "        :func:`torch.nonzero(..., as_tuple=False) <torch.nonzero>` (default) returns a\n",
      "        2-D tensor where each row is the index for a nonzero value.\n",
      "    \n",
      "        :func:`torch.nonzero(..., as_tuple=True) <torch.nonzero>` returns a tuple of 1-D\n",
      "        index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]``\n",
      "        gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor\n",
      "        contains nonzero indices for a certain dimension.\n",
      "    \n",
      "        See below for more details on the two behaviors.\n",
      "    \n",
      "        When :attr:`input` is on CUDA, :func:`torch.nonzero() <torch.nonzero>` causes\n",
      "        host-device synchronization.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``False`` **(default)**:\n",
      "    \n",
      "    Returns a tensor containing the indices of all non-zero elements of\n",
      "    :attr:`input`.  Each row in the result contains the indices of a non-zero\n",
      "    element in :attr:`input`. The result is sorted lexicographically, with\n",
      "    the last index changing the fastest (C-style).\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor\n",
      "    :attr:`out` is of size :math:`(z \\times n)`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    **When** :attr:`as_tuple` **is** ``True``:\n",
      "    \n",
      "    Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`,\n",
      "    each containing the indices (in that dimension) of all non-zero elements of\n",
      "    :attr:`input` .\n",
      "    \n",
      "    If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n`\n",
      "    tensors of size :math:`z`, where :math:`z` is the total number of\n",
      "    non-zero elements in the :attr:`input` tensor.\n",
      "    \n",
      "    As a special case, when :attr:`input` has zero dimensions and a nonzero scalar\n",
      "    value, it is treated as a one-dimensional tensor with one element.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "    \n",
      "    Keyword args:\n",
      "        out (LongTensor, optional): the output tensor containing indices\n",
      "    \n",
      "    Returns:\n",
      "        LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output\n",
      "        tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for\n",
      "        each dimension, containing the indices of each nonzero element along that\n",
      "        dimension.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
      "        tensor([[ 0],\n",
      "                [ 1],\n",
      "                [ 2],\n",
      "                [ 4]])\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]))\n",
      "        tensor([[ 0,  0],\n",
      "                [ 1,  1],\n",
      "                [ 2,  2],\n",
      "                [ 3,  3]])\n",
      "        >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 4]),)\n",
      "        >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "        ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "        ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n",
      "        (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
      "        >>> torch.nonzero(torch.tensor(5), as_tuple=True)\n",
      "        (tensor([0]),)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. torch.nonzero(input, as_tuple)\n",
    "# https://blog.csdn.net/wangxuecheng666/article/details/120639138\n",
    "# shape为Z*N（Z是非0的数的个数，N为input的维数）\n",
    "help(torch.nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e42b9837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [0, 3, 3]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 返回非0数对应的index\n",
    "x = torch.nonzero(torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]]))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9ecc5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6000,  0.4000,  0.5000,  1.2000, -0.4000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2返回多维innput中非零的数\n",
    "x = torch.tensor([[[0.6, 0.0, 0.0, 0.0],\n",
    "                                [0.0, 0.4, 0.5, 0.0],\n",
    "                                [0.0, 0.0, 1.2, 0.0],\n",
    "                                [0.0, 0.0, 0.0,-0.4]]])\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f46677c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f92899",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3返回一维innput中非零的数\n",
    "x = torch.tensor([5,3])\n",
    "print(x)\n",
    "x[x.nonzero(as_tuple=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "471a8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function gather in module torch:\n",
      "\n",
      "gather(...)\n",
      "    gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor\n",
      "    \n",
      "    Gathers values along an axis specified by `dim`.\n",
      "    \n",
      "    For a 3-D tensor the output is specified by::\n",
      "    \n",
      "        out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
      "        out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
      "        out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
      "    \n",
      "    :attr:`input` and :attr:`index` must have the same number of dimensions.\n",
      "    It is also required that ``index.size(d) <= input.size(d)`` for all\n",
      "    dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.\n",
      "    Note that ``input`` and ``index`` do not broadcast against each other.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the source tensor\n",
      "        dim (int): the axis along which to index\n",
      "        index (LongTensor): the indices of elements to gather\n",
      "    \n",
      "    Keyword arguments:\n",
      "        sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.\n",
      "        out (Tensor, optional): the destination tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> t = torch.tensor([[1, 2], [3, 4]])\n",
      "        >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\n",
      "        tensor([[ 1,  1],\n",
      "                [ 4,  3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. torch.gather(input, dim, index)\n",
    "# https://blog.csdn.net/Apikaqiu/article/details/104253080\n",
    "help(torch.gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb1f3a5f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8294865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 0, torch.tensor([[0, 0], [1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf53ee22",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(a, 0,torch.tensor([8,4,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da92d43",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17],\n",
       "        [18, 19, 20, 21, 22, 23],\n",
       "        [24, 25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34, 35]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(36).reshape(6,6)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d20458",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 19, 26],\n",
       "        [ 0,  7, 14]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(b, 0, torch.tensor([[0,3,4],\n",
    "                                [0,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18fa7ac0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 4],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[[0, 3, 4],\n",
    "[6, 7,8]]\n",
    "\"\"\"\n",
    "torch.gather(b, 1, torch.tensor([[0,3,4],\n",
    "                                [0,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "919c2bce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10, 11],\n",
       "         [12, 13, 14, 15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20, 21, 22, 23],\n",
       "         [24, 25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34, 35]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.arange(36).reshape(2,3,6)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93a1d3c3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 0,  7, 14]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(c,1, torch.tensor([[[0,0,0],\n",
    "                   [0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cf087fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 0,  7, 14]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [18, 25, 32]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(c,1, torch.tensor([[[0,0,0],\n",
    "                   [0,1,2]],\n",
    "                               [[0,0,0],\n",
    "                   [0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08b796fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 1, 2],\n",
       "        [0, 2, 2],\n",
       "        [0, 3, 3]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7ba6a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(15)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12566eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 用view()来改变Tensor的形状\n",
    "\"\"\"\n",
    "注意view()返回的新tensor与源Tensor虽然可能有不同的size,但是共享data，也即是更改其中的一个，另外一个也会跟着改变。\n",
    "（顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变）\n",
    "\"\"\"\n",
    "y = x.view(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29e0b8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6a8c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.view(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "18380e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd8dcf4d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32cf667e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2275ace",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a52f5b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n",
      "tensor([[-1,  0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12, 13]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "# 8. clone().view()\n",
    "\"\"\"\n",
    "如果我们想返回一个真正的副本（即不共享data内存）该怎么办呢？---clone().view()\n",
    "pytorch还提供了一个reshape()苦役改变形状，但是此函数并不能保证返回的是其拷贝，\n",
    "所以不推荐使用.\n",
    "推荐先用clone创造一个副本然后再使用view\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor\n",
    "\"\"\"\n",
    "print(z)\n",
    "z_cp = z.clone().view(15)\n",
    "print(z_cp)\n",
    "z -= 1\n",
    "print(z)\n",
    "print(z_cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd9a16ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9166])\n",
      "1.9165549278259277\n"
     ]
    }
   ],
   "source": [
    "# 9. item()\n",
    "\"\"\"\n",
    "可以将一个标量转换成一个python number\n",
    "\"\"\"\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9f4e8a7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6424, -1.8490,  0.2794],\n",
       "        [ 0.8115, -0.0530, -0.6954]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = torch.randn(2,3)\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786b169",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10. #######线性代数相关的一些函数##############\n",
    "\"\"\"\n",
    "trace   对角线元素之和(矩阵的迹)\n",
    "diag  对角线元素\n",
    "triu/tril 矩阵的上三角/下三角，可指定偏移量\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b92e59ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function trace in module torch:\n",
      "\n",
      "trace(...)\n",
      "    trace(input) -> Tensor\n",
      "    \n",
      "    Returns the sum of the elements of the diagonal of the input 2-D matrix.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.arange(1., 10.).view(3, 3)\n",
      "        >>> x\n",
      "        tensor([[ 1.,  2.,  3.],\n",
      "                [ 4.,  5.,  6.],\n",
      "                [ 7.,  8.,  9.]])\n",
      "        >>> torch.trace(x)\n",
      "        tensor(15.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e57f9500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function diag in module torch:\n",
      "\n",
      "diag(...)\n",
      "    diag(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor\n",
      "      with the elements of :attr:`input` as the diagonal.\n",
      "    - If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with\n",
      "      the diagonal elements of :attr:`input`.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider:\n",
      "    \n",
      "    - If :attr:`diagonal` = 0, it is the main diagonal.\n",
      "    - If :attr:`diagonal` > 0, it is above the main diagonal.\n",
      "    - If :attr:`diagonal` < 0, it is below the main diagonal.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "            :func:`torch.diagonal` always returns the diagonal of its input.\n",
      "    \n",
      "            :func:`torch.diagflat` always constructs a tensor with diagonal elements\n",
      "            specified by the input.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    Get the square matrix where the input vector is the diagonal::\n",
      "    \n",
      "        >>> a = torch.randn(3)\n",
      "        >>> a\n",
      "        tensor([ 0.5950,-0.0872, 2.3298])\n",
      "        >>> torch.diag(a)\n",
      "        tensor([[ 0.5950, 0.0000, 0.0000],\n",
      "                [ 0.0000,-0.0872, 0.0000],\n",
      "                [ 0.0000, 0.0000, 2.3298]])\n",
      "        >>> torch.diag(a, 1)\n",
      "        tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n",
      "                [ 0.0000, 0.0000,-0.0872, 0.0000],\n",
      "                [ 0.0000, 0.0000, 0.0000, 2.3298],\n",
      "                [ 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "    \n",
      "    Get the k-th diagonal of a given matrix::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[-0.4264, 0.0255,-0.1064],\n",
      "                [ 0.8795,-0.2429, 0.1374],\n",
      "                [ 0.1029,-0.6482,-1.6300]])\n",
      "        >>> torch.diag(a, 0)\n",
      "        tensor([-0.4264,-0.2429,-1.6300])\n",
      "        >>> torch.diag(a, 1)\n",
      "        tensor([ 0.0255, 0.1374])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "input是一维，output就是二维， input是二维，则output是一维\n",
    "\"\"\"\n",
    "help(torch.diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a66cb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function triu in module torch:\n",
      "\n",
      "triu(...)\n",
      "    triu(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices\n",
      "    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.\n",
      "    \n",
      "    The upper triangular part of the matrix is defined as the elements on and\n",
      "    above the diagonal.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider. If\n",
      "    :attr:`diagonal` = 0, all elements on and above the main diagonal are\n",
      "    retained. A positive value excludes just as many diagonals above the main\n",
      "    diagonal, and similarly a negative value includes just as many diagonals below\n",
      "    the main diagonal. The main diagonal are the set of indices\n",
      "    :math:`\\lbrace (i, i) \\rbrace` for :math:`i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]` where\n",
      "    :math:`d_{1}, d_{2}` are the dimensions of the matrix.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.2072, -1.0680,  0.6602],\n",
      "                [ 0.3480, -0.5211, -0.4573]])\n",
      "        >>> torch.triu(a)\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.0000, -1.0680,  0.6602],\n",
      "                [ 0.0000,  0.0000, -0.4573]])\n",
      "        >>> torch.triu(a, diagonal=1)\n",
      "        tensor([[ 0.0000,  0.5207,  2.0049],\n",
      "                [ 0.0000,  0.0000,  0.6602],\n",
      "                [ 0.0000,  0.0000,  0.0000]])\n",
      "        >>> torch.triu(a, diagonal=-1)\n",
      "        tensor([[ 0.2309,  0.5207,  2.0049],\n",
      "                [ 0.2072, -1.0680,  0.6602],\n",
      "                [ 0.0000, -0.5211, -0.4573]])\n",
      "    \n",
      "        >>> b = torch.randn(4, 6)\n",
      "        >>> b\n",
      "        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n",
      "                [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n",
      "        >>> torch.triu(b, diagonal=1)\n",
      "        tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n",
      "                [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n",
      "        >>> torch.triu(b, diagonal=-1)\n",
      "        tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n",
      "                [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n",
      "                [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n",
      "                [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ead34a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function tril in module torch:\n",
      "\n",
      "tril(...)\n",
      "    tril(input, diagonal=0, *, out=None) -> Tensor\n",
      "    \n",
      "    Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices\n",
      "    :attr:`input`, the other elements of the result tensor :attr:`out` are set to 0.\n",
      "    \n",
      "    The lower triangular part of the matrix is defined as the elements on and\n",
      "    below the diagonal.\n",
      "    \n",
      "    The argument :attr:`diagonal` controls which diagonal to consider. If\n",
      "    :attr:`diagonal` = 0, all elements on and below the main diagonal are\n",
      "    retained. A positive value includes just as many diagonals above the main\n",
      "    diagonal, and similarly a negative value excludes just as many diagonals below\n",
      "    the main diagonal. The main diagonal are the set of indices\n",
      "    :math:`\\lbrace (i, i) \\rbrace` for :math:`i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]` where\n",
      "    :math:`d_{1}, d_{2}` are the dimensions of the matrix.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor.\n",
      "        diagonal (int, optional): the diagonal to consider\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> a = torch.randn(3, 3)\n",
      "        >>> a\n",
      "        tensor([[-1.0813, -0.8619,  0.7105],\n",
      "                [ 0.0935,  0.1380,  2.2112],\n",
      "                [-0.3409, -0.9828,  0.0289]])\n",
      "        >>> torch.tril(a)\n",
      "        tensor([[-1.0813,  0.0000,  0.0000],\n",
      "                [ 0.0935,  0.1380,  0.0000],\n",
      "                [-0.3409, -0.9828,  0.0289]])\n",
      "    \n",
      "        >>> b = torch.randn(4, 6)\n",
      "        >>> b\n",
      "        tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n",
      "                [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n",
      "                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n",
      "                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n",
      "        >>> torch.tril(b, diagonal=1)\n",
      "        tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n",
      "                [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n",
      "                [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n",
      "        >>> torch.tril(b, diagonal=-1)\n",
      "        tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "                [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67732357",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mm in module torch:\n",
      "\n",
      "mm(...)\n",
      "    mm(input, mat2, *, out=None) -> Tensor\n",
      "    \n",
      "    Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.\n",
      "    \n",
      "    If :attr:`input` is a :math:`(n \\times m)` tensor, :attr:`mat2` is a\n",
      "    :math:`(m \\times p)` tensor, :attr:`out` will be a :math:`(n \\times p)` tensor.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "    \n",
      "    Supports strided and sparse 2-D tensors as inputs, autograd with\n",
      "    respect to strided inputs.\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the first matrix to be matrix multiplied\n",
      "        mat2 (Tensor): the second matrix to be matrix multiplied\n",
      "    \n",
      "    Keyword args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> mat1 = torch.randn(2, 3)\n",
      "        >>> mat2 = torch.randn(3, 3)\n",
      "        >>> torch.mm(mat1, mat2)\n",
      "        tensor([[ 0.4851,  0.5037, -0.3633],\n",
      "                [-0.0760, -3.6705,  2.4784]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n",
    "# 数学上的矩阵相乘\n",
    "help(torch.mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9801f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2621,  2.3984, -1.6417],\n",
       "        [ 0.9421,  0.0194, -0.1172]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50eb61bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3501, -0.9045],\n",
       "        [ 1.0299, -0.0246],\n",
       "        [-0.8473,  0.7765]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = torch.randn(3, 2)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4796bafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.2151, -1.5709],\n",
       "        [ 1.3911, -0.9436]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28a2c251",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.256522970000001"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5752*1.6299 + 1.2822*2.9772 -0.6495*0.7673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "540bc451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function bmm in module torch:\n",
      "\n",
      "bmm(...)\n",
      "    bmm(input, mat2, *, out=None) -> Tensor\n",
      "    \n",
      "    Performs a batch matrix-matrix product of matrices stored in :attr:`input`\n",
      "    and :attr:`mat2`.\n",
      "    \n",
      "    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing\n",
      "    the same number of matrices.\n",
      "    \n",
      "    If :attr:`input` is a :math:`(b \\times n \\times m)` tensor, :attr:`mat2` is a\n",
      "    :math:`(b \\times m \\times p)` tensor, :attr:`out` will be a\n",
      "    :math:`(b \\times n \\times p)` tensor.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n",
      "    \n",
      "    This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "    \n",
      "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "    \n",
      "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the first batch of matrices to be multiplied\n",
      "        mat2 (Tensor): the second batch of matrices to be multiplied\n",
      "    \n",
      "    Keyword Args:\n",
      "        out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> input = torch.randn(10, 3, 4)\n",
      "        >>> mat2 = torch.randn(10, 4, 5)\n",
      "        >>> res = torch.bmm(input, mat2)\n",
      "        >>> res.size()\n",
      "        torch.Size([10, 3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mm/bmm  矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm   矩阵运算\n",
    "t  转置\n",
    "dot/cross 内积/外积\n",
    "inverse 求逆矩阵\n",
    "svd 奇异值分解\n",
    "\"\"\"\n",
    "help(torch.bmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0af4f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:tensor([[[-1.1847,  0.0410, -0.5768,  0.4169],\n",
      "         [-0.6415, -0.3177, -0.8972, -0.7950],\n",
      "         [-2.9986,  2.6437, -1.8537,  0.4957]],\n",
      "\n",
      "        [[-1.2137,  2.3454, -0.1044, -0.6831],\n",
      "         [ 0.3544,  0.2728,  0.8918,  0.5072],\n",
      "         [ 0.0171, -1.3940,  1.0164, -0.0145]]])\n",
      "mat2:tensor([[[ 0.6089,  0.5453, -0.9820,  0.2671, -1.3822],\n",
      "         [ 0.2006,  0.2895, -0.9884,  2.1604,  2.5285],\n",
      "         [-1.4788, -2.0407,  0.2791, -0.0694, -0.5121],\n",
      "         [-0.3281, -1.0097, -0.3885,  0.8986,  0.5167]],\n",
      "\n",
      "        [[-1.3075, -0.0859,  0.2601,  0.7127, -0.1840],\n",
      "         [-0.2685, -0.1976, -1.5441, -1.3401,  0.8365],\n",
      "         [-0.8632, -0.5366,  0.3598, -0.3461, -1.2170],\n",
      "         [-0.5274, -1.4887,  0.3305, -0.3523,  1.0441]]])\n",
      "res:tensor([[[ 3.1184e-03,  1.2203e-01,  7.9981e-01,  1.8691e-01,  2.2520e+00],\n",
      "         [ 1.1333e+00,  2.1919e+00,  1.0024e+00, -1.5099e+00,  1.3191e-01],\n",
      "         [ 1.2830e+00,  2.4125e+00, -3.7852e-01,  5.4848e+00,  1.2035e+01]],\n",
      "\n",
      "        [[ 1.4076e+00,  7.1383e-01, -4.2006e+00, -3.7312e+00,  1.5991e+00],\n",
      "         [-1.5739e+00, -1.3180e+00,  1.5947e-01, -6.0040e-01, -3.9279e-01],\n",
      "         [-5.1782e-01, -2.4990e-01,  2.5179e+00,  1.5336e+00, -2.4213e+00]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(2, 3, 4)\n",
    "print(f\"input:{input}\")\n",
    "mat2 = torch.randn(2, 4, 5)\n",
    "print(f\"mat2:{mat2}\")\n",
    "res = torch.bmm(input, mat2)\n",
    "print(f\"res:{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09f9277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030477199999999205"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1.1847) *0.6089  + 0.0410*0.2006 + (-0.5768)*(-1.4788) + 0.4169*(-0.3281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29488c35",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13987eae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50836d2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cc075",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18442cd9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb993d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4a457",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254c4ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "67ba705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. 用方法to()可以将Tensor在CPU和GPU(需要硬件支持)之间相互移动\n",
    "\"\"\"\n",
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n",
    "\"\"\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60747446",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy=torch.ones_like(yy)\n",
    "yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "113e1073",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = yyy.to(\"cpu\", torch.double)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f8000",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
