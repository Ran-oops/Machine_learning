{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b08f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSoftmax函数存在于数学中，尤其是概率论和相关领域。它能将一个含任意实数的K维向量“压缩”到另一个K维实向量中，使得每一个元素的范围都在0和1之间，并且所有元素的和为1。\\n这个函数通常按下面的式子给出：Softmax(zi\\u200b)=∑j=1K\\u200bezj\\u200bezi\\u200b\\u200b\\n\\n举个例子，假设我们有一个三维向量[1.0, 2.0, 3.0]，\\n我们可以使用Softmax函数将其转换为概率分布。首先，我们计算每个元素的指数：\\n[e^1.0, e^2.0, e^3.0]，然后计算这些指数的和：e^1.0 + e^2.0 + e^3.0。最后，\\n我们将每个元素的指数除以指数之和，得到结果：[e^1.0 / (e^1.0 + e^2.0 + e^3.0), \\ne^2.0 / (e^1.0 + e^2.0 + e^3.0), e^3.0 / (e^1.0 + e^2.0 + e^3.0)]，\\n约等于[0.09, 0.24, 0.67]。这就是Softmax函数将原始向量转换为概率分布的过程。\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Softmax函数存在于数学中，尤其是概率论和相关领域。它能将一个含任意实数的K维向量“压缩”到另一个K维实向量中，使得每一个元素的范围都在0和1之间，并且所有元素的和为1。\n",
    "这个函数通常按下面的式子给出：Softmax(zi​)=∑j=1K​ezj​ezi​​\n",
    "\n",
    "举个例子，假设我们有一个三维向量[1.0, 2.0, 3.0]，\n",
    "我们可以使用Softmax函数将其转换为概率分布。首先，我们计算每个元素的指数：\n",
    "[e^1.0, e^2.0, e^3.0]，然后计算这些指数的和：e^1.0 + e^2.0 + e^3.0。最后，\n",
    "我们将每个元素的指数除以指数之和，得到结果：[e^1.0 / (e^1.0 + e^2.0 + e^3.0), \n",
    "e^2.0 / (e^1.0 + e^2.0 + e^3.0), e^3.0 / (e^1.0 + e^2.0 + e^3.0)]，\n",
    "约等于[0.09, 0.24, 0.67]。这就是Softmax函数将原始向量转换为概率分布的过程。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faee9c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n机器学习实践者用分类这个词来描述两个微妙差别的问题： 1. 我们只对样本的“硬性”\\n类别感兴趣， 即属于哪个类别；\\n\\n2. 我们希望得到“软性”类别，即得到属于每个类别的概率\\n\\n这两者的界限往往很模糊，其中的一个原因是：即使我们只关心硬类别， 我们仍然\\n使用软类别的模型\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "机器学习实践者用分类这个词来描述两个微妙差别的问题： 1. 我们只对样本的“硬性”\n",
    "类别感兴趣， 即属于哪个类别；\n",
    "\n",
    "2. 我们希望得到“软性”类别，即得到属于每个类别的概率\n",
    "\n",
    "这两者的界限往往很模糊，其中的一个原因是：即使我们只关心硬类别， 我们仍然\n",
    "使用软类别的模型\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75563520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n我们从一个图像分类问题开始。 假设每次输入时一个2x2的灰度图像，我们可以用一个\\n标量表示每个像素值；每个图像对应四个特征x1, x2, x3, x4. 此外，假设每个图像\\n属于类别“猫”， “鸡” 和 “狗”中的一个\\n\\n接下来，我们要选择如何表示标签。我们有两个明显的选择： 最直接的想法是选择\\ny∈{1,2,3},其中整数分别代表{狗， 猫， 鸡}。这是在计算机上存储此类信息的有效方法，\\n如果类别间有一些自然顺序，比如我们试图预测{婴儿， 儿童， 青少年， 青年人， 中年人， 老年人}，\\n那么将这个问题转变为回归问题，并且保留这种格式是有意义的。\\n\\n但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就\\n发明了一种表示分类数据的简单方法： 独热编码(one-hot encoding). 独热编码是一个\\n向量，它的分量和类别一样多，类别对应的分量设置为1， 其他所有分量设置为0.\\n在我们的例子中，标签y是一个三维向量，其中(1,0,0)对应“猫”， (0,1,0)对应于“鸡”，\\n(0,0,1)对应于“狗”\\n\\ny∈{(1, 0, 0), (0, 1, 0), (0, 0, 1)}\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============1. 分类问题: 独热编码============================\n",
    "\n",
    "\"\"\"\n",
    "我们从一个图像分类问题开始。 假设每次输入时一个2x2的灰度图像，我们可以用一个\n",
    "标量表示每个像素值；每个图像对应四个特征x1, x2, x3, x4. 此外，假设每个图像\n",
    "属于类别“猫”， “鸡” 和 “狗”中的一个\n",
    "\n",
    "接下来，我们要选择如何表示标签。我们有两个明显的选择： 最直接的想法是选择\n",
    "y∈{1,2,3},其中整数分别代表{狗， 猫， 鸡}。这是在计算机上存储此类信息的有效方法，\n",
    "如果类别间有一些自然顺序，比如我们试图预测{婴儿， 儿童， 青少年， 青年人， 中年人， 老年人}，\n",
    "那么将这个问题转变为回归问题，并且保留这种格式是有意义的。\n",
    "\n",
    "但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就\n",
    "发明了一种表示分类数据的简单方法： 独热编码(one-hot encoding). 独热编码是一个\n",
    "向量，它的分量和类别一样多，类别对应的分量设置为1， 其他所有分量设置为0.\n",
    "在我们的例子中，标签y是一个三维向量，其中(1,0,0)对应“猫”， (0,1,0)对应于“鸡”，\n",
    "(0,0,1)对应于“狗”\n",
    "\n",
    "y∈{(1, 0, 0), (0, 1, 0), (0, 0, 1)}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============2. 网格架构： 仿射函数， 全连接层============================\n",
    "\"\"\"\n",
    "为了估计所有可能类别的条件概率， 我们需要一个或多个输出的模型， 每个类别对应\n",
    "一个输出。 为了解决线性模型的分类问题， 我们需要和输出一样多的仿射函数(affine function).\n",
    "每个输出对应于它自己的仿射函数。对于此例， 我们有4个特征和3个可能得输出类别，\n",
    "我们将需要12个标量来表示权重(带下标的w), 3个标量来表示偏置(带下标的b)。 下面\n",
    "我们为每个输入计算三个未规范化的预测(logit): o1, o2, o3\n",
    "o1 = x1w11 +x2w12 + x3w13 + x4w14 + b1\n",
    "o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2\n",
    "03 = x1w31 +x2w32 + x3w33 +x4w34 + b3\n",
    "\n",
    "我们可以用神经网络图来描述这个计算过程。 与线性回归一样， softmax回归也是\n",
    "单层神经网络。 由于计算每个输出o1, o2和o3取决于所有输入x1, x2, x3和x4， 所以\n",
    "softmax回归的输出层也是全连接层\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa7237f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax_net.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"softmax_net.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"====表达式与线性回归的表达式也不一样，线性的是y_hat = Xw +b \n",
    "为了更简洁地表达模型， 我们仍然使用线性代数符号，通过向量形式表达为\n",
    "o = Wx + b, 这是一种更适合数学和编写代码的形式。由此，我们已经将所有权重\n",
    "放到一个3x4的矩阵中， 对于给定数据样本的特征x,我们的输出是由权重与输入特征进行\n",
    "矩阵-向量乘法再加上偏置b得到的。\"\"\"\n",
    "\n",
    "\"\"\"=============不同在于，softmax回归的输出值个数等于标签里的类别数===========\n",
    "softmax回归跟线性回归一样将输入特征与权重做线性叠加。 与线性回归的一个主要\n",
    "不同在于，softmax回归的输出值个数等于标签里的类别数。 因为一共由4种特征和3种\n",
    "输出动物类别。所以权重包含12个标量（带下标的w）, 偏差包含3个标量(带下标的b),\n",
    "且对每个输入计算o1, o2, o3这3个输出：\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"============softmax运算符(softmax operator)的作用: 正数， 和为1========\n",
    "既然分类问题需要离散的预测输出，一个简单的办法就是将输出值oi当作预测类别是i\n",
    "的置信度，并将值最大的输出所对应的类作为预测输出，即输出arg maxoi. 例如， 如果\n",
    "o1， o2, o3分别为0.1， 10， 0.1， 由于o2最大， 那么预测类别为2， 其代表猫。\n",
    "\n",
    "然而， 直接使用输出层的输出有两个问题， 一方面，由于输出层的输出值的范围不确定，\n",
    "我们难以直观上判定这些值的意义。例如， 刚才举的例子中的输出值10表示“很置信”图像\n",
    "为猫， 因为该输出值是其他两类的输出值的100倍， 但如果o1= o3=10^3,  那么输出值\n",
    "10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值， 这些离散值\n",
    "与不确定范围的输出值之间的的误差难以衡量。\n",
    "\n",
    "\n",
    "softmax运算符（softmax operator）解决了以上两个问题， 它通过下式将输出值变换\n",
    "为值为正且和为1的概率分布。\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be04cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax_formula1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"softmax_formula1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========3. 单样本分类的矢量计算表达式=========\n",
    "为了提高计算效率， 我们可以将单样本分类通过矢量计算来表达。 在上面的图像分类问题中，\n",
    "假设softmax回归的权重和偏差参数分别为\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41efbeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax_矢量表达式.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"softmax_矢量表达式.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========4. 小批量样本分类的矢量计算表达式=========\n",
    "为了进一步提升计算效率， 我们通常对小批量数据做矢量计算。广义上讲， 给定一个小批量\n",
    "样本， 其批量大小为n, 输入个数(特征数)为d, 输出个数(类别数)为q, 设\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88852143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========5. 全连接层的参数开销=========\n",
    "全连接层是“完全”连接的， 可能有很多可学习的参数。 具体来说， 对于任何具有d个\n",
    "输入和q个输出的全连接层， 参数开销为O(dq), 这个数字在实践中可能高得令人望而却步。\n",
    "幸运的是， 将d个输入转换为q个输出的成本可以减少到O(dq/n). 其中超参数n可以由我们\n",
    "灵活指定， 以在实际应用中平衡参数节约和模型有效性。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be999fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argmax是一种函数，是对函数求参数 (集合)的函数。 当我们有另一个函数y=f (x)时，若有结果x0= argmax (f (x))，则表示当函数f (x)取x=x0的时候，得到f (x)取值范围的最大值；若有多个点使得f (x)取得相同的最大值，那么argmax (f (x))的结果就是一个点集。换句话说，argmax (f (x))是使得 f (x)取得最大值所对应的\n",
    "变量点x (或x的集合)1。\n",
    "\n",
    "举个例子来说，假设我们有一个函数f(x) = -x^2 + 4x + 5，那么我们可以通过\n",
    "求导来找到这个函数的最大值。求导后得到f’(x) = -2x + 4，令f’(x) = 0，\n",
    "解得x = 2。所以当x = 2时，f(x)取得最大值。\n",
    "因此，argmax(f(x)) = 2。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4288fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features: \n",
      " [[ 0.88371925 -0.21618405  1.31999908]\n",
      " [ 0.24465228  0.67573301 -0.31874941]]\n",
      "Weights: \n",
      " [[-0.33149279 -1.68281227]\n",
      " [-0.33729463 -1.1121145 ]\n",
      " [ 0.75282378  0.49702314]]\n",
      "Biases: \n",
      " [0. 0.]\n",
      "Unnormalized predictions: \n",
      " [[ 0.77369786 -0.59064209]\n",
      " [-0.54898372 -1.32162216]]\n",
      "Output probabilities: \n",
      " [[0.79646415 0.20353585]\n",
      " [0.68409137 0.31590863]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = np.exp(X)\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "batch_size = 2\n",
    "num_inputs = 3\n",
    "num_outputs = 2\n",
    "\n",
    "X = np.random.normal(size=(batch_size, num_inputs))\n",
    "W = np.random.normal(size=(num_inputs, num_outputs))\n",
    "b = np.zeros(num_outputs)\n",
    "\n",
    "O = np.dot(X, W) + b\n",
    "Y_hat = softmax(O)\n",
    "\n",
    "print(\"Batch features: \\n\", X)\n",
    "print(\"Weights: \\n\", W)\n",
    "print(\"Biases: \\n\", b)\n",
    "print(\"Unnormalized predictions: \\n\", O)\n",
    "print(\"Output probabilities: \\n\", Y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4affa78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cross_entropy_loss_function.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"=========6. 交叉熵损失函数=========\n",
    "交叉熵损失函数是一种常用于分类问题的损失函数。它衡量了两个概率分布之间的\n",
    "差异，即真实概率分布与预测概率分布之间的差异。 在图像分类问题中，为了计算网络\n",
    "的损失，模型的输出药确保归一化到0到1之间。二分类问题通常使用sigmoid函数来\n",
    "进行归一化，多分类问题通常使用softmax函数来归一化。交叉熵损失函数的具体公式如下\n",
    "所示：其中p表示真实概率分布， q表示预测概率分布，n表示类别数量。这个公式\n",
    "可以用于二分类和多分类问题。交叉熵损失函数在神经网络做分类问题时也经常\n",
    "使用，它几乎每次都和sigmoid或softmax函数一起出现。\n",
    "\n",
    "\"\"\"\n",
    "Image(url=\"cross_entropy_loss_function.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4e43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"sigmoid_function.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"=========7. Sigmoid函数=========\n",
    "Sigmoid函数（S型函数/S型生长曲线）：在深度学习中，由于其单增以及反函数单增\n",
    "等性质，Sigmoid函数常被用作神经网络的激活函数，将变量映射到[0,1]之间。它可以\n",
    "将一个实数映射到(0,1)之间，可以用来做二分类。\n",
    "Sigmoid函数的具体公式为：\n",
    "\"\"\"\n",
    "Image(url=\"sigmoid_function.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========8. 模型预测及评价========\n",
    "在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。\n",
    "通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别(标签)一致， 说明\n",
    "这次预测时正确的。我们使用准确率(accuracy)来评价模型的表现。 它等于正确\n",
    "预测数量与总预测数量之比\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbbc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=========9. 小结========\n",
    "softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。\n",
    "softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数\n",
    "交叉熵适合衡量两个概率分布的差异\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
