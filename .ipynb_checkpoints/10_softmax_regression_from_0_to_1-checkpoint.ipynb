{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95126b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b501500",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1. 初始化模型参数\n",
    "原始数据集中的每个样本都是28x28的图像。本节将展开每个图像，把它们看作长度为\n",
    "784的向量。\n",
    "我们的输出与类别一样多。因为我们的数据集由10个类别， 所以网络输出维度为10\n",
    "权重将构成一个784x10的矩阵，偏置将构成一个1x10的行向量。与线性回归一样，我们\n",
    "将使用正态分布初始化我们的权重W， 偏置初始化为0\n",
    "\"\"\"\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1ded4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0014, -0.0025, -0.0275,  ..., -0.0090,  0.0121, -0.0010],\n",
       "        [-0.0081,  0.0085,  0.0048,  ...,  0.0122,  0.0051, -0.0110],\n",
       "        [ 0.0055, -0.0236, -0.0032,  ...,  0.0021,  0.0169, -0.0133],\n",
       "        ...,\n",
       "        [-0.0060, -0.0196,  0.0082,  ..., -0.0103,  0.0181, -0.0146],\n",
       "        [ 0.0086, -0.0014, -0.0005,  ..., -0.0045, -0.0115, -0.0113],\n",
       "        [-0.0010, -0.0081,  0.0025,  ..., -0.0058, -0.0222, -0.0005]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964d2cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20ef7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 7., 9.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"2. 定义softmax操作\n",
    "在\n",
    "实现softmax回归模型之前，我们简要回顾一下sum运算符如何沿着张量中的特定维度工作。\n",
    "给定一个矩阵X，我们可以对所有元素求和（默认情况下）。也可以只求同一个轴上\n",
    "的元素，即同一列（轴0）或同一行（轴1）。 如果X是一个形状为(2, 3)的张量，我们对\n",
    "列进行求和，则结果将是一个具有形状（3，）的向量。当调用sum运算符时，我们可以指定\n",
    "保持在原始张量的轴数，而不折叠求和的维度。这将产生一个形状(1,3)的二维张量\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "X.sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7ef8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [15.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67793fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax_formula.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "softmax由3个步骤组成：\n",
    "1. 对每个项求幂（使用exp）\n",
    "2. 对每一行求和(小批量中每个样本是一行)， 得到每个样本的规范化常数；\n",
    "3. 将每一行除以其规范化常数，确保结果的和为1\n",
    "\"\"\"\n",
    "from IPython.display import Image\n",
    "Image(url=\"softmax_formula.png\")\n",
    "\n",
    "\"\"\"\n",
    "分母或规范化常数，有时也称为配分函数（其对数称为对数-配分函数）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1062cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86685c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9584, -1.7195, -0.0439,  0.5639,  1.1817],\n",
       "        [ 0.7076, -1.1241,  0.4916,  0.1014,  0.1994]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.normal(0, 1, (2,5))\n",
    "X_prob = softmax(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516673a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0587, 0.0274, 0.1464, 0.2689, 0.4987],\n",
       "        [0.3213, 0.0514, 0.2588, 0.1752, 0.1933]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576bd1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prob.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56941e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3. 定义模型\n",
    "定义softmax操作后，我们可以实现softmax回归模型。下面的代码定义了输入如何通过\n",
    "网络映射到输出。注意，将数据传递到模型之前，我们使用reshape函数将每张原始图像\n",
    "展平为向量\n",
    "\"\"\"\n",
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6eed823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"4. 定义损失函数\n",
    "交叉熵损失函数，这可能是深度学习中最常见的损失函数，因为目前分类问题的数量\n",
    "远远超过回归问题的数量。\n",
    "\n",
    "交叉熵采用真实标签的预测概率的负对数似然。 这里我们不使用Python的for循环\n",
    "迭代预测(这往往是低效的)， 而是通过一个运算符选择所有元素。 下面，我们创建\n",
    "一个数据样本y_hat, 其中包含2个样本在3个类别的预测概率，以及它们对应的标签y.\n",
    "有了y, 我们知道在第一个样本中，第一类是正确的预测； 而在第二个样本中， 第三类\n",
    "是正确的预测。然后使用y和y_hat中概率的索引， 我们选择第一个样本中第一个类别\n",
    "的概率和第二个样本中第三个类的概率。\n",
    "\"\"\"\n",
    "\n",
    "y = torch.tensor([0,2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3,0.2,0.5]])\n",
    "y_hat[[0,1], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea575c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.3000, 0.6000],\n",
       "        [0.3000, 0.2000, 0.5000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b233f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.3000, 0.6000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df2b1875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da1c3b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[[0], 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01ff40d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实现交叉熵损失函数\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5. 分类精度\n",
    "给定预测概率分布y_hat，当我们必须输出硬预测（hard prediction）时， 我们通常\n",
    "选择预测概率最高的类。\n",
    "\n",
    "当预测与标签分类y一致时，即是正确的。分类精度即正确预测数量与总预测数量之比。\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
